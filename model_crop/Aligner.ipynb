{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, random, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.data import imread\n",
    "from skimage.io import imshow,imsave\n",
    "from skimage import img_as_float\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.util import crop\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = '../data/train/'\n",
    "TEST_DIR = '../data/test_stg1/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "modelStr = 'Crop'\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "BatchSize = 64\n",
    "LearningRate = 1e-4\n",
    "le = LabelEncoder()\n",
    "le.fit(FISH_CLASSES)\n",
    "le.transform(FISH_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rescale=1./255,\n",
    "rotation_range=180,\n",
    "shear_range=0.2,\n",
    "zoom_range=[0.9,1.1],\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "horizontal_flip=True,\n",
    "vertical_flip=True\n",
    "\n",
    "def random_transform(image,\n",
    "                     rotation_range=0.,\n",
    "                     width_shift_range=0.,\n",
    "                     height_shift_range=0.,\n",
    "                     shear_range=0.,\n",
    "                     zoom_range=0.,\n",
    "                     horizontal_flip=False,\n",
    "                     vertical_flip=False,\n",
    "                     rescale=None,):\n",
    "    # x is a single image, so it doesn't have image number at index 0\n",
    "    img_row_axis = self.row_axis - 1\n",
    "    img_col_axis = self.col_axis - 1\n",
    "    img_channel_axis = self.channel_axis - 1\n",
    "\n",
    "    # use composition of homographies\n",
    "    # to generate final transform that needs to be applied\n",
    "    if rotation_range:\n",
    "        theta = np.pi / 180 * np.random.uniform(-self.rotation_range, self.rotation_range)\n",
    "    else:\n",
    "        theta = 0\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]])\n",
    "    if height_shift_range:\n",
    "        tx = np.random.uniform(-self.height_shift_range, self.height_shift_range) * image.shape[0]\n",
    "    else:\n",
    "        tx = 0\n",
    "\n",
    "    if width_shift_range:\n",
    "        ty = np.random.uniform(-self.width_shift_range, self.width_shift_range) * image.shape[1]\n",
    "    else:\n",
    "        ty = 0\n",
    "\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                   [0, 1, ty],\n",
    "                                   [0, 0, 1]])\n",
    "    if shear_range:\n",
    "        shear = np.random.uniform(-self.shear_range, self.shear_range)\n",
    "    else:\n",
    "        shear = 0\n",
    "    shear_matrix = np.array([[1, -np.sin(shear), 0],\n",
    "                             [0, np.cos(shear), 0],\n",
    "                             [0, 0, 1]])\n",
    "\n",
    "    if zoom_range[0] == 1 and zoom_range[1] == 1:\n",
    "        zx, zy = 1, 1\n",
    "    else:\n",
    "        zx, zy = np.random.uniform(self.zoom_range[0], self.zoom_range[1], 2)\n",
    "    zoom_matrix = np.array([[zx, 0, 0],\n",
    "                            [0, zy, 0],\n",
    "                            [0, 0, 1]])\n",
    "\n",
    "    transform_matrix = np.dot(np.dot(np.dot(rotation_matrix,\n",
    "                                            translation_matrix),\n",
    "                                     shear_matrix),\n",
    "                              zoom_matrix)\n",
    "\n",
    "    h, w = x.shape[img_row_axis], x.shape[img_col_axis]\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)\n",
    "    x = apply_transform(x, transform_matrix, img_channel_axis,\n",
    "                        fill_mode=self.fill_mode, cval=self.cval)\n",
    "\n",
    "    if self.horizontal_flip:\n",
    "        if np.random.random() < 0.5:\n",
    "            x = flip_axis(x, img_col_axis)\n",
    "\n",
    "    if self.vertical_flip:\n",
    "        if np.random.random() < 0.5:\n",
    "            x = flip_axis(x, img_row_axis)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-033d661aa6f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deg_angle_between(x1,y1,x2,y2):\n",
    "    from math import atan2, degrees, pi\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    rads = atan2(dy,dx)\n",
    "    #rads %= 2*pi\n",
    "    degs = degrees(rads)\n",
    "    return(degs)\n",
    "\n",
    "def get_rotated_cropped_fish(image,x1,y1,x2,y2):\n",
    "    (h,w) = image.shape[:2]\n",
    "    #calculate center and angle\n",
    "    center = ( (x1+x2) / 2,(y1+y2) / 2)\n",
    "    angle = np.floor(deg_angle_between(x1,y1,x2,y2))\n",
    "    #print('angle=' +str(angle) + ' ')\n",
    "    #print('center=' +str(center))\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    fish_length = np.sqrt((x1-x2)**2+(y1-y2)**2)\n",
    "    cropped = rotated[(max((center[1]-fish_length/1.8),0)):(max((center[1]+fish_length/1.8),0)) ,\n",
    "                      (max((center[0]- fish_length/1.8),0)):(max((center[0]+fish_length/1.8),0))]\n",
    "    #imshow(image)\n",
    "    #imshow(rotated)\n",
    "    #imshow(cropped)\n",
    "    resized = resize(cropped,(ROWS,COLS))\n",
    "    return(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist data_train_annoCrop_224_224.pickle. Loading data from file.\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('../data/data_train_annoCrop_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_train_annoCrop_{}_{}.pickle. Loading data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_train_annoCrop_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    X_train = data_train['X_train']\n",
    "    y_train = data_train['y_train']\n",
    "    del data_train\n",
    "else:\n",
    "    print ('Loading data from original images. Generating data_train_annoCrop_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "    \n",
    "    images = []\n",
    "    y_train = []\n",
    "\n",
    "    crop_classes=FISH_CLASSES[:]\n",
    "    crop_classes.remove('NoF')\n",
    "    crop_classes\n",
    "\n",
    "    for c in crop_classes:\n",
    "        labels = pd.read_json('../data/annotation/'+c.lower()+'_labels.json')\n",
    "        for i in range(len(labels)):\n",
    "            try:\n",
    "                img_filename = labels.iloc[i,2]\n",
    "                print(img_filename)\n",
    "                l1 = pd.DataFrame((labels[labels.filename==img_filename].annotations).iloc[0])\n",
    "                image = imread(TRAIN_DIR+c+'/'+img_filename)\n",
    "                images.append(get_rotated_cropped_fish(image,np.floor(l1.iloc[0,1]),np.floor(l1.iloc[0,2]),np.floor(l1.iloc[1,1]),np.floor(l1.iloc[1,2])))\n",
    "                print('success')\n",
    "                y_train.append(c)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    X_train = np.asarray(images, dtype=np.float32)\n",
    "\n",
    "    # One Hot Encoding Labels\n",
    "    y_train = le.transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "    #save data to file\n",
    "    data_train = {'X_train': X_train,'y_train': y_train }\n",
    "\n",
    "    with open('../data/data_train_annoCrop_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_train, f)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=None, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=180,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "#train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BatchSize, shuffle=True, seed=None)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator = valid_datagen.flow(X_valid, y_valid, batch_size=BatchSize, shuffle=True, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='./checkpoints/weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        \n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 512/2639 [====>.........................] - ETA: 3499s - loss: 1.7124 - acc: 0.5293"
     ]
    }
   ],
   "source": [
    "#stg1 training\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional VGG16 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "hist = model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stg2 training\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:14]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[14:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "hist = model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#resume training\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "hist = model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test submission\n",
    "\n",
    "import datetime\n",
    "\n",
    "if os.path.exists('../data/data_test_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_test_{}_{}.pickle. Loading test data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_test_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "    X_test = data_test['X_test']\n",
    "    test_files = data_test['test_files']\n",
    "else:\n",
    "    print ('Loading test data from original images. Generating data_test_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    test_files = [im for im in os.listdir(TEST_DIR)]\n",
    "    X_test = np.ndarray((len(test_files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(test_files): \n",
    "        X_test[i] = read_image(TEST_DIR+im)\n",
    "        if i%300 == 0: print('Processed {} of {}'.format(i, len(test_files)))\n",
    "            \n",
    "    data_test = {'X_test': X_test,'test_files': test_files }\n",
    "    \n",
    "    with open('../data/data_test_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_test, f)\n",
    "            \n",
    "X_test = X_test / 255.\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "model = load_model(files[index])\n",
    "\n",
    "test_preds = model.predict(X_test, batch_size=BatchSize, verbose=1)\n",
    "#test_preds= test_preds / np.sum(test_preds,axis=1,keepdims=True)\n",
    "\n",
    "submission = pd.DataFrame(test_preds, columns=FISH_CLASSES)\n",
    "#submission.loc[:, 'image'] = pd.Series(test_files, index=submission.index)\n",
    "submission.insert(0, 'image', test_files)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "info = modelStr + '{:.4f}'.format(min(val_losses))\n",
    "sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "submission.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###clear checkpoints folder\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "files = glob.glob('./checkpoints/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###clear logs folder\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')\n",
    "files = glob.glob('./logs/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['acc']); plt.plot(hist.history['val_acc']);\n",
    "plt.title('model accuracy'); plt.ylabel('accuracy');\n",
    "plt.xlabel('epoch'); plt.legend(['train', 'valid'], loc='upper left');\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist.history['loss']); plt.plot(hist.history['val_loss']);\n",
    "plt.title('model loss'); plt.ylabel('loss');\n",
    "plt.xlabel('epoch'); plt.legend(['train', 'valid'], loc='upper left');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
