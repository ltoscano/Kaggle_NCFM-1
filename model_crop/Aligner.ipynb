{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, random, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.data import imread\n",
    "from skimage.io import imshow,imsave\n",
    "from skimage import img_as_float\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.util import crop\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = '../data/train/'\n",
    "TEST_DIR = '../data/test_stg1/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "modelStr = 'Crop'\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "BatchSize = 64\n",
    "LearningRate = 1e-4\n",
    "le = LabelEncoder()\n",
    "le.fit(FISH_CLASSES)\n",
    "le.transform(FISH_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#生成图像随机变换矩阵\n",
    "#modified from code https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py\n",
    "\n",
    "def transform_matrix_offset_center(matrix, w, h):\n",
    "    center_x = float(w) / 2 + 0.5\n",
    "    center_y = float(h) / 2 + 0.5\n",
    "    #图像center移到原点，进行rotation和shear\n",
    "    offset_matrix = np.array([[1, 0, center_x], [0, 1, center_y], [0, 0, 1]])\n",
    "    #移回来\n",
    "    reset_matrix = np.array([[1, 0, -center_x], [0, 1, -center_y], [0, 0, 1]])\n",
    "    transform_matrix = np.dot(np.dot(reset_matrix, matrix), offset_matrix)\n",
    "    return transform_matrix\n",
    "\n",
    "def random_transform_matrix(image,\n",
    "                            rotation_range=0.,\n",
    "                            width_shift_range=0.,\n",
    "                            height_shift_range=0.,\n",
    "                            shear_range=0.,\n",
    "                            zoom_range=0.,\n",
    "                            horizontal_flip=False,\n",
    "                            vertical_flip=False):\n",
    "    \n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "       \n",
    "    #图像上下翻转\n",
    "    hflip_matrix=np.eye(3)\n",
    "    if horizontal_flip:\n",
    "        if np.random.random() < 0.5:\n",
    "            print(\"horizontal_flip\")\n",
    "            hflip_matrix = np.array([[-1, 0, w],\n",
    "                                     [0, 1, 0],\n",
    "                                     [0, 0, 1]])\n",
    "    #图像左右翻转                              \n",
    "    vflip_matrix=np.eye(3)\n",
    "    if vertical_flip:\n",
    "        if np.random.random() < 0.5:\n",
    "            print(\"vertical_flip\")\n",
    "            vflip_matrix = np.array([[1, 0, 0],\n",
    "                                     [0, -1, h],\n",
    "                                     [0, 0, 1]])\n",
    "    #图像顺时针旋转theta       \n",
    "    if rotation_range:\n",
    "        theta = np.pi / 180 * np.random.uniform(-rotation_range, rotation_range)\n",
    "    else:\n",
    "        theta = 0\n",
    "    print(\"theta =\",theta)\n",
    "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                [np.sin(theta), np.cos(theta), 0],\n",
    "                                [0, 0, 1]])\n",
    "    \n",
    "    #图像往正轴移动tx，ty\n",
    "    if height_shift_range:\n",
    "        ty = np.random.uniform(-height_shift_range, height_shift_range) * h\n",
    "    else:\n",
    "        ty = 0\n",
    "\n",
    "    if width_shift_range:\n",
    "        tx = np.random.uniform(-width_shift_range, width_shift_range) * w\n",
    "    else:\n",
    "        tx = 0\n",
    "    print(\"tx =\",tx)\n",
    "    print(\"ty =\",ty)\n",
    "    translation_matrix = np.array([[1, 0, tx],\n",
    "                                   [0, 1, ty],\n",
    "                                   [0, 0, 1]])\n",
    "    \n",
    "    #图像顺时针shear\n",
    "    if shear_range:\n",
    "        shear = np.random.uniform(-shear_range, shear_range)\n",
    "    else:\n",
    "        shear = 0\n",
    "    print(\"shear =\",shear)\n",
    "    shear_matrix = np.array([[1, -np.sin(shear), 0],\n",
    "                             [0, np.cos(shear), 0],\n",
    "                             [0, 0, 1]])\n",
    "    \n",
    "    #以center为中心图像放大zx，zy\n",
    "    if np.isscalar(zoom_range):\n",
    "        zoom_range = [1 - zoom_range, 1 + zoom_range]\n",
    "    elif len(zoom_range) == 2:\n",
    "        zoom_range = [zoom_range[0], zoom_range[1]]\n",
    "    else:\n",
    "        raise ValueError('zoom_range should be a float or '\n",
    "                         'a tuple or list of two floats. '\n",
    "                         'Received arg: ', zoom_range)\n",
    "            \n",
    "    if zoom_range[0] == 1 and zoom_range[1] == 1:\n",
    "        zx, zy = 1, 1\n",
    "    else:\n",
    "        zx, zy = np.random.uniform(zoom_range[0], zoom_range[1], 2)\n",
    "    print(\"zx =\",zx)\n",
    "    print(\"zy =\",zy)\n",
    "    zoom_matrix = np.array([[zx, 0, (1-zx)*w/2.],\n",
    "                            [0, zy, (1-zy)*h/2.],\n",
    "                            [0, 0, 1]])\n",
    "    #transform_matrix = zoom_matrix\n",
    "    transform_matrix = np.dot(shear_matrix, rotation_matrix)\n",
    "    transform_matrix = transform_matrix_offset_center(transform_matrix, w, h)\n",
    "    transform_matrix = np.dot(np.dot(np.dot(np.dot(translation_matrix, \n",
    "                                                   zoom_matrix), \n",
    "                                            transform_matrix), \n",
    "                                     vflip_matrix), \n",
    "                              hflip_matrix)\n",
    "    return transform_matrix[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_json('../data/annotation/alb_labels.json')\n",
    "img_filename = labels.iloc[0,2]\n",
    "print(img_filename)\n",
    "l1 = pd.DataFrame((labels[labels.filename==img_filename].annotations).iloc[0])\n",
    "x_head = l1.iloc[0,1]\n",
    "y_head = l1.iloc[0,2]\n",
    "x_tail = l1.iloc[1,1]\n",
    "y_tail = l1.iloc[1,2]\n",
    "\n",
    "image = imread(TRAIN_DIR+'ALB/'+img_filename)\n",
    "rescale = 1./255,\n",
    "image = image*rescale\n",
    "h, w = image.shape[0], image.shape[1]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.plot(x_head, y_head,'rs')\n",
    "plt.plot(x_tail, y_tail,'gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = random_transform_matrix(image,\n",
    "                            rotation_range=20,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.1,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True)\n",
    "image_transformed = cv2.warpAffine(image, M, (w, h), borderMode=1)\n",
    "head_transformed = np.dot(M,np.array([x_head,y_head,1]))\n",
    "tail_transformed = np.dot(M,np.array([x_tail,y_tail,1]))\n",
    "x_head_transformed = head_transformed[0]\n",
    "y_head_transformed = head_transformed[1]\n",
    "x_tail_transformed = tail_transformed[0]\n",
    "y_tail_transformed = tail_transformed[1]\n",
    "\n",
    "plt.imshow(image_transformed)\n",
    "if 0<=x_head_transformed<=w and 0<=y_head_transformed<=h:\n",
    "    plt.plot(x_head_transformed, y_head_transformed,'rs')\n",
    "if 0<=x_tail_transformed<=w and 0<=y_tail_transformed<=h:\n",
    "    plt.plot(x_tail_transformed, y_tail_transformed,'gs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "center = ( (x_head_transformed+x_tail_transformed) / 2,(y_head_transformed+y_tail_transformed) / 2)\n",
    "fish_length = np.sqrt((x_tail_transformed-x_head_transformed)**2+\n",
    "                      (y_tail_transformed-y_head_transformed)**2)\n",
    "image_cropped = image_transformed[(max((center[1]-fish_length/1.8),0)):(max((center[1]+fish_length/1.8),0)) ,\n",
    "                  (max((center[0]- fish_length/1.8),0)):(max((center[0]+fish_length/1.8),0))]\n",
    "\n",
    "plt.imshow(image_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_resized = resize(image_transformed,(ROWS,COLS))\n",
    "plt.imshow(image_resized)\n",
    "x_ratio = float(COLS)/w\n",
    "y_ratio = float(ROWS)/h\n",
    "print(x_ratio,y_ratio)\n",
    "head_resized = [head_transformed[0]*x_ratio,head_transformed[1]*y_ratio]\n",
    "tail_resized = [tail_transformed[0]*x_ratio,tail_transformed[1]*y_ratio]\n",
    "plt.plot(head_resized[0], head_resized[1],'rs')\n",
    "plt.plot(tail_resized[0], tail_resized[1],'gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "crop_classes=FISH_CLASSES[:]\n",
    "crop_classes.remove('NoF')\n",
    "crop_classes\n",
    "\n",
    "images = []\n",
    "y_train = []\n",
    "\n",
    "for e in range(100):\n",
    "    for c in crop_classes:\n",
    "        labels = pd.read_json('../data/annotation/'+c.lower()+'_labels.json')\n",
    "        for i in range(len(labels)):\n",
    "            try:\n",
    "                img_filename = labels.iloc[i,2]\n",
    "                print(img_filename)\n",
    "                l1 = pd.DataFrame((labels[labels.filename==img_filename].annotations).iloc[0])\n",
    "                image = imread(TRAIN_DIR+c+'/'+img_filename)\n",
    "                images.append(get_rotated_cropped_fish(image,np.floor(l1.iloc[0,1]),np.floor(l1.iloc[0,2]),np.floor(l1.iloc[1,1]),np.floor(l1.iloc[1,2])))\n",
    "                print('success')\n",
    "                y_train.append(c)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "model.train_on_batch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('../data/data_train_annotation_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_train_annotation_{}_{}.pickle. Loading data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_train_annotation_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    X_train = data_train['X_train']\n",
    "    y_train = data_train['y_train']\n",
    "    del data_train\n",
    "else:\n",
    "    print ('Loading data from original images. Generating data_train_annotation_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "    \n",
    "    images = []\n",
    "    y_train = []\n",
    "\n",
    "    crop_classes=FISH_CLASSES[:]\n",
    "    crop_classes.remove('NoF')\n",
    "    crop_classes\n",
    "\n",
    "    for c in crop_classes:\n",
    "        labels = pd.read_json('../data/annotation/'+c.lower()+'_labels.json')\n",
    "        for i in range(len(labels)):\n",
    "            try:\n",
    "                img_filename = labels.iloc[i,2]\n",
    "                print(img_filename)\n",
    "                l1 = pd.DataFrame((labels[labels.filename==img_filename].annotations).iloc[0])\n",
    "                image = imread(TRAIN_DIR+c+'/'+img_filename)\n",
    "                rescale = 1./255,\n",
    "                image = image*rescale\n",
    "                images.append(image)\n",
    "                y_train.append([l1.iloc[0,1],l1.iloc[0,2],l1.iloc[1,1],l1.iloc[1,2]])\n",
    "                print('success')\n",
    "            except:\n",
    "                print('fail')\n",
    "\n",
    "    X_train = np.asarray(images, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.float32)\n",
    "\n",
    "    #save data to file\n",
    "    data_train = {'X_train': X_train,'y_train': y_train }\n",
    "\n",
    "    with open('../data/data_train_annotation_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_train, f)\n",
    "\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=None, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=180,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "#train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BatchSize, shuffle=True, seed=None)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator = valid_datagen.flow(X_valid, y_valid, batch_size=BatchSize, shuffle=True, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='./checkpoints/weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        \n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stg1 training\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional VGG16 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "hist = model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stg2 training\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:14]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[14:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "hist = model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#resume training\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "hist = model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test submission\n",
    "\n",
    "import datetime\n",
    "\n",
    "if os.path.exists('../data/data_test_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_test_{}_{}.pickle. Loading test data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_test_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "    X_test = data_test['X_test']\n",
    "    test_files = data_test['test_files']\n",
    "else:\n",
    "    print ('Loading test data from original images. Generating data_test_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    test_files = [im for im in os.listdir(TEST_DIR)]\n",
    "    X_test = np.ndarray((len(test_files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(test_files): \n",
    "        X_test[i] = read_image(TEST_DIR+im)\n",
    "        if i%300 == 0: print('Processed {} of {}'.format(i, len(test_files)))\n",
    "            \n",
    "    data_test = {'X_test': X_test,'test_files': test_files }\n",
    "    \n",
    "    with open('../data/data_test_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_test, f)\n",
    "            \n",
    "X_test = X_test / 255.\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "model = load_model(files[index])\n",
    "\n",
    "test_preds = model.predict(X_test, batch_size=BatchSize, verbose=1)\n",
    "#test_preds= test_preds / np.sum(test_preds,axis=1,keepdims=True)\n",
    "\n",
    "submission = pd.DataFrame(test_preds, columns=FISH_CLASSES)\n",
    "#submission.loc[:, 'image'] = pd.Series(test_files, index=submission.index)\n",
    "submission.insert(0, 'image', test_files)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "info = modelStr + '{:.4f}'.format(min(val_losses))\n",
    "sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "submission.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###clear checkpoints folder\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "files = glob.glob('./checkpoints/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###clear logs folder\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')\n",
    "files = glob.glob('./logs/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['acc']); plt.plot(hist.history['val_acc']);\n",
    "plt.title('model accuracy'); plt.ylabel('accuracy');\n",
    "plt.xlabel('epoch'); plt.legend(['train', 'valid'], loc='upper left');\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist.history['loss']); plt.plot(hist.history['val_loss']);\n",
    "plt.title('model loss'); plt.ylabel('loss');\n",
    "plt.xlabel('epoch'); plt.legend(['train', 'valid'], loc='upper left');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
