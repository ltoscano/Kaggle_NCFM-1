{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name ReduceLROnPlateau",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-97cd1e90d3f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name ReduceLROnPlateau"
     ]
    }
   ],
   "source": [
    "import os, random, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.data import imread\n",
    "from skimage.io import imshow,imsave\n",
    "from skimage import img_as_float\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.util import crop\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = '../data/train/'\n",
    "TEST_DIR = '../data/test_stg1/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "modelStr = 'Crop'\n",
    "ROWS = 224\n",
    "COLS = 224\n",
    "BatchSize = 64\n",
    "LearningRate = 1e-4\n",
    "le = LabelEncoder()\n",
    "le.fit(FISH_CLASSES)\n",
    "le.transform(FISH_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'img_00003.jpg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deg_angle_between(x1,y1,x2,y2):\n",
    "    from math import atan2, degrees, pi\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    rads = atan2(dy,dx)\n",
    "    rads %= 2*pi\n",
    "    degs = degrees(rads)\n",
    "    return(degs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'class': u'point', u'x': 825.5028464692997, u'y': 342.8499725255559},\n",
       " {u'class': u'point', u'x': 1095.1227277758048, u'y': 449.36646884417524}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rotated_cropped_fish(img,x1,y1,x2,y2):\n",
    "    (h,w) = img.shape[:2]\n",
    "    #calculate center and angle\n",
    "    center = ( (x1+x2) / 2,(y1+y2) / 2)\n",
    "    angle = np.floor(deg_angle_between(x1,y1,x2,y2))\n",
    "    print('angle=' +str(angle) + ' ')\n",
    "    print('center=' +str(center))\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "    \n",
    "    fish_length = np.sqrt((x1-x2)**2+(y1-y2)**2)\n",
    "    cropped = rotated[(max((center[1]-fish_length/1.8),0)):(max((center[1]+fish_length/1.8),0)) ,\n",
    "                      (max((center[0]- fish_length/1.8),0)):(max((center[0]+fish_length/1.8),0))]\n",
    "    imshow(img)\n",
    "    imshow(rotated)\n",
    "    imshow(cropped)\n",
    "    resized = resize(cropped,(ROWS,COLS))\n",
    "    return(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>class</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{u'y': 342.849972526, u'x': 825.502846469, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00003.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations  class       filename\n",
       "1  [{u'y': 342.849972526, u'x': 825.502846469, u'...  image  img_00003.jpg"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_files = ['/home/nati/data/kaggle/fish/input/train/BET/bet_labels.json',\n",
    "             '/home/nati/data/kaggle/fish/input/train/ALB/alb_labels.json',\n",
    "             '/home/nati/data/kaggle/fish/input/train/YFT/yft_labels.json',\n",
    "             '/home/nati/data/kaggle/fish/input/train/DOL/dol_labels.json',\n",
    "             '/home/nati/data/kaggle/fish/input/train/SHARK/shark_labels.json',\n",
    "             '/home/nati/data/kaggle/fish/input/train/LAG/lag_labels.json',\n",
    "             '/home/nati/data/kaggle/fish/input/train/OTHER/other_labels.json']\n",
    "\n",
    "data_dirs = ['/home/nati/data/kaggle/fish/input/train/BET/',\n",
    "             '/home/nati/data/kaggle/fish/input/train/ALB/',\n",
    "             '/home/nati/data/kaggle/fish/input/train/YFT/',\n",
    "             '/home/nati/data/kaggle/fish/input/train/DOL/',\n",
    "             '/home/nati/data/kaggle/fish/input/train/SHARK/',\n",
    "             '/home/nati/data/kaggle/fish/input/train/LAG/',\n",
    "             '/home/nati/data/kaggle/fish/input/train/OTHER/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>class</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{u'y': 155.0, u'x': 409.0, u'class': u'point'}]</td>\n",
       "      <td>image</td>\n",
       "      <td>image2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{u'y': 342.849972526, u'x': 825.502846469, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{u'y': 606.090168807, u'x': 721.615642971, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00010.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{u'y': 514.004645922, u'x': 492.238976876, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00012.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{u'y': 359.55, u'x': 258.03, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00015.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{u'y': 459.2511, u'x': 165.0123, u'class': u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00019.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{u'y': 675.171053793, u'x': 712.942860998, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00020.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{u'y': 458.25, u'x': 713.46, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00029.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{u'y': 553.344341256, u'x': 526.033354456, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{u'y': 280.234473254, u'x': 307.545460054, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00037.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[{u'y': 391.853288872, u'x': 814.579867171, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00038.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[{u'y': 185.239736558, u'x': 400.165328333, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00039.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[{u'y': 213.044796, u'x': 796.114764, u'class'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00041.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[{u'y': 493.972630821, u'x': 579.467893847, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00043.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[{u'y': 437.588652482, u'x': 763.120567376, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00045.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[{u'y': 408.524865162, u'x': 190.868174707, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00055.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[{u'y': 512.765957447, u'x': 476.595744681, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00057.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[{u'y': 148.0, u'x': 418.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00074.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[{u'y': 214.7148, u'x': 536.787, u'class': u'p...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00085.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[{u'y': 129.2265, u'x': 375.7509, u'class': u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00090.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[{u'y': 522.8703, u'x': 467.2035, u'class': u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00097.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[{u'y': 113.3217, u'x': 475.1559, u'class': u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00110.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[{u'y': 381.494820243, u'x': 546.556596772, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00121.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[{u'y': 155.808704721, u'x': 779.043523607, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00130.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[{u'y': 403.157834553, u'x': 379.266999913, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00134.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[{u'y': 683.9064, u'x': 165.0123, u'class': u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00136.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[{u'y': 478.014184397, u'x': 303.546099291, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00154.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[{u'y': 277.77, u'x': 368.01, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00156.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[{u'y': 397.62, u'x': 423.0, u'class': u'point...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00163.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[{u'y': 413.5248, u'x': 852.8949, u'class': u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_00177.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>[{u'y': 443.0, u'x': 402.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07807.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>[{u'y': 203.913868113, u'x': 661.758587115, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07810.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>[{u'y': 508.284670282, u'x': 444.707638785, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07811.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>[{u'y': 357.970680369, u'x': 181.216739176, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07814.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>[{u'y': 242.376666095, u'x': 312.787906746, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07822.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>[{u'y': 229.702383861, u'x': 260.862310781, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07826.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>[{u'y': 515.678928156, u'x': 249.099948302, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07836.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>[{u'y': 173.151656313, u'x': 532.326755762, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07850.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>[{u'y': 81.5602836879, u'x': 602.836879433, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07851.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>[{u'y': 408.477367794, u'x': 549.78203863, u'c...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07858.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>[{u'y': 581.842762265, u'x': 562.843814926, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07862.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>[{u'y': 243.424012784, u'x': 745.708683066, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07863.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>[{u'y': 179.302565514, u'x': 787.268880371, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07868.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>[{u'y': 381.166380994, u'x': 584.217630682, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07869.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>[{u'y': 616.17, u'x': 624.63, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07871.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>[{u'y': 397.62, u'x': 427.23, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07874.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>[{u'y': 597.84, u'x': 714.87, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07876.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>[{u'y': 98.7, u'x': 609.12, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07877.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>[{u'y': 540.03, u'x': 1085.7, u'class': u'poin...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07878.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>[{u'y': 435.0, u'x': 550.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07879.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>[{u'y': 501.097236073, u'x': 607.966314856, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07883.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>[{u'y': 157.0, u'x': 776.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07885.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>[{u'y': 600.0, u'x': 712.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07890.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>[{u'y': 166.0, u'x': 271.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07892.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>[{u'y': 472.0, u'x': 647.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07903.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>[{u'y': 499.909801864, u'x': 148.429276088, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07904.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>[{u'y': 288.0, u'x': 396.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07912.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>[{u'y': 261.235525915, u'x': 358.605131029, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07914.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>[{u'y': 489.222893986, u'x': 469.036512438, u'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07915.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>[{u'y': 212.0, u'x': 557.0, u'class': u'point'...</td>\n",
       "      <td>image</td>\n",
       "      <td>img_07917.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1720 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            annotations  class       filename\n",
       "0      [{u'y': 155.0, u'x': 409.0, u'class': u'point'}]  image     image2.jpg\n",
       "1     [{u'y': 342.849972526, u'x': 825.502846469, u'...  image  img_00003.jpg\n",
       "2     [{u'y': 606.090168807, u'x': 721.615642971, u'...  image  img_00010.jpg\n",
       "3     [{u'y': 514.004645922, u'x': 492.238976876, u'...  image  img_00012.jpg\n",
       "4     [{u'y': 359.55, u'x': 258.03, u'class': u'poin...  image  img_00015.jpg\n",
       "5     [{u'y': 459.2511, u'x': 165.0123, u'class': u'...  image  img_00019.jpg\n",
       "6     [{u'y': 675.171053793, u'x': 712.942860998, u'...  image  img_00020.jpg\n",
       "7     [{u'y': 458.25, u'x': 713.46, u'class': u'poin...  image  img_00029.jpg\n",
       "8     [{u'y': 553.344341256, u'x': 526.033354456, u'...  image  img_00032.jpg\n",
       "9     [{u'y': 280.234473254, u'x': 307.545460054, u'...  image  img_00037.jpg\n",
       "10    [{u'y': 391.853288872, u'x': 814.579867171, u'...  image  img_00038.jpg\n",
       "11    [{u'y': 185.239736558, u'x': 400.165328333, u'...  image  img_00039.jpg\n",
       "12    [{u'y': 213.044796, u'x': 796.114764, u'class'...  image  img_00041.jpg\n",
       "13    [{u'y': 493.972630821, u'x': 579.467893847, u'...  image  img_00043.jpg\n",
       "14    [{u'y': 437.588652482, u'x': 763.120567376, u'...  image  img_00045.jpg\n",
       "15    [{u'y': 408.524865162, u'x': 190.868174707, u'...  image  img_00055.jpg\n",
       "16    [{u'y': 512.765957447, u'x': 476.595744681, u'...  image  img_00057.jpg\n",
       "17    [{u'y': 148.0, u'x': 418.0, u'class': u'point'...  image  img_00074.jpg\n",
       "18    [{u'y': 214.7148, u'x': 536.787, u'class': u'p...  image  img_00085.jpg\n",
       "19    [{u'y': 129.2265, u'x': 375.7509, u'class': u'...  image  img_00090.jpg\n",
       "20    [{u'y': 522.8703, u'x': 467.2035, u'class': u'...  image  img_00097.jpg\n",
       "21    [{u'y': 113.3217, u'x': 475.1559, u'class': u'...  image  img_00110.jpg\n",
       "22    [{u'y': 381.494820243, u'x': 546.556596772, u'...  image  img_00121.jpg\n",
       "23    [{u'y': 155.808704721, u'x': 779.043523607, u'...  image  img_00130.jpg\n",
       "24    [{u'y': 403.157834553, u'x': 379.266999913, u'...  image  img_00134.jpg\n",
       "25    [{u'y': 683.9064, u'x': 165.0123, u'class': u'...  image  img_00136.jpg\n",
       "26    [{u'y': 478.014184397, u'x': 303.546099291, u'...  image  img_00154.jpg\n",
       "27    [{u'y': 277.77, u'x': 368.01, u'class': u'poin...  image  img_00156.jpg\n",
       "28    [{u'y': 397.62, u'x': 423.0, u'class': u'point...  image  img_00163.jpg\n",
       "29    [{u'y': 413.5248, u'x': 852.8949, u'class': u'...  image  img_00177.jpg\n",
       "...                                                 ...    ...            ...\n",
       "1690  [{u'y': 443.0, u'x': 402.0, u'class': u'point'...  image  img_07807.jpg\n",
       "1691  [{u'y': 203.913868113, u'x': 661.758587115, u'...  image  img_07810.jpg\n",
       "1692  [{u'y': 508.284670282, u'x': 444.707638785, u'...  image  img_07811.jpg\n",
       "1693  [{u'y': 357.970680369, u'x': 181.216739176, u'...  image  img_07814.jpg\n",
       "1694  [{u'y': 242.376666095, u'x': 312.787906746, u'...  image  img_07822.jpg\n",
       "1695  [{u'y': 229.702383861, u'x': 260.862310781, u'...  image  img_07826.jpg\n",
       "1696  [{u'y': 515.678928156, u'x': 249.099948302, u'...  image  img_07836.jpg\n",
       "1697  [{u'y': 173.151656313, u'x': 532.326755762, u'...  image  img_07850.jpg\n",
       "1698  [{u'y': 81.5602836879, u'x': 602.836879433, u'...  image  img_07851.jpg\n",
       "1699  [{u'y': 408.477367794, u'x': 549.78203863, u'c...  image  img_07858.jpg\n",
       "1700  [{u'y': 581.842762265, u'x': 562.843814926, u'...  image  img_07862.jpg\n",
       "1701  [{u'y': 243.424012784, u'x': 745.708683066, u'...  image  img_07863.jpg\n",
       "1702  [{u'y': 179.302565514, u'x': 787.268880371, u'...  image  img_07868.jpg\n",
       "1703  [{u'y': 381.166380994, u'x': 584.217630682, u'...  image  img_07869.jpg\n",
       "1704  [{u'y': 616.17, u'x': 624.63, u'class': u'poin...  image  img_07871.jpg\n",
       "1705  [{u'y': 397.62, u'x': 427.23, u'class': u'poin...  image  img_07874.jpg\n",
       "1706  [{u'y': 597.84, u'x': 714.87, u'class': u'poin...  image  img_07876.jpg\n",
       "1707  [{u'y': 98.7, u'x': 609.12, u'class': u'point'...  image  img_07877.jpg\n",
       "1708  [{u'y': 540.03, u'x': 1085.7, u'class': u'poin...  image  img_07878.jpg\n",
       "1709  [{u'y': 435.0, u'x': 550.0, u'class': u'point'...  image  img_07879.jpg\n",
       "1710  [{u'y': 501.097236073, u'x': 607.966314856, u'...  image  img_07883.jpg\n",
       "1711  [{u'y': 157.0, u'x': 776.0, u'class': u'point'...  image  img_07885.jpg\n",
       "1712  [{u'y': 600.0, u'x': 712.0, u'class': u'point'...  image  img_07890.jpg\n",
       "1713  [{u'y': 166.0, u'x': 271.0, u'class': u'point'...  image  img_07892.jpg\n",
       "1714  [{u'y': 472.0, u'x': 647.0, u'class': u'point'...  image  img_07903.jpg\n",
       "1715  [{u'y': 499.909801864, u'x': 148.429276088, u'...  image  img_07904.jpg\n",
       "1716  [{u'y': 288.0, u'x': 396.0, u'class': u'point'...  image  img_07912.jpg\n",
       "1717  [{u'y': 261.235525915, u'x': 358.605131029, u'...  image  img_07914.jpg\n",
       "1718  [{u'y': 489.222893986, u'x': 469.036512438, u'...  image  img_07915.jpg\n",
       "1719  [{u'y': 212.0, u'x': 557.0, u'class': u'point'...  image  img_07917.jpg\n",
       "\n",
       "[1720 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = list()\n",
    "labels_list = list()\n",
    "for c in range(7):\n",
    "    labels = pd.read_json(label_files[c])\n",
    "    for i in range(len(labels)):\n",
    "        try:\n",
    "            img_filename = labels.iloc[i,2]\n",
    "            print(img_filename)\n",
    "            l1 = pd.DataFrame((labels[labels.filename==img_filename].annotations).iloc[0])\n",
    "            image = imread(data_dirs[c]+img_filename)\n",
    "            images.append(get_rotated_cropped_fish(image,np.floor(l1.iloc[0,1]),np.floor(l1.iloc[0,2]),np.floor(l1.iloc[1,1]),np.floor(l1.iloc[1,2])))\n",
    "            print('success')\n",
    "            labels_list.append(c)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 5, 2, 6, 4, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(['LAG', 'YFT', 'OTHER', 'DOL', 'SHARK', 'NoF', 'BET', 'ALB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist data_train_224_224.pickle. Loading data from file.\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "import pickle\n",
    "\n",
    "def get_images(fish):\n",
    "    \"\"\"Load files from train folder\"\"\"\n",
    "    fish_dir = TRAIN_DIR+'{}'.format(fish)\n",
    "    images = [fish+'/'+im for im in os.listdir(fish_dir)]\n",
    "    return images\n",
    "\n",
    "def read_image(src):\n",
    "    \"\"\"Read and resize individual images\"\"\"\n",
    "    im = Image.open(src)\n",
    "    im = im.resize((COLS, ROWS), Image.BILINEAR)\n",
    "    im = np.asarray(im)\n",
    "    return im\n",
    "    \n",
    "if os.path.exists('../data/data_train_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_train_{}_{}.pickle. Loading data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_train_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    X_train = data_train['X_train']\n",
    "    y_train = data_train['y_train']\n",
    "else:\n",
    "    print ('Loading data from original images. Generating data_train_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    files = []\n",
    "    y_train = []\n",
    "\n",
    "    for fish in FISH_CLASSES:\n",
    "        fish_files = get_images(fish)\n",
    "        files.extend(fish_files)\n",
    "\n",
    "        y_fish = np.tile(fish, len(fish_files))\n",
    "        y_train.extend(y_fish)\n",
    "        #print(\"{0} photos of {1}\".format(len(fish_files), fish))\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    X_train = np.ndarray((len(files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(files): \n",
    "        X_train[i] = read_image(TRAIN_DIR+im)\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "    #X_train = X_train / 255.\n",
    "    #print(X_train.shape)\n",
    "\n",
    "    # One Hot Encoding Labels\n",
    "    y_train = le.transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "    #save data to file\n",
    "    data_train = {'X_train': X_train,'y_train': y_train }\n",
    "\n",
    "    with open('../data/data_train_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_train, f)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=None, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=180,\n",
    "    shear_range=np.pi/6.,\n",
    "    zoom_range=[1,1.1],\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "#train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BatchSize, shuffle=True, seed=None)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator = valid_datagen.flow(X_valid, y_valid, batch_size=BatchSize, shuffle=True, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='./checkpoints/weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        \n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.7936 - acc: 0.3873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from 1.54322 to 1.53578, saving model to ./checkpoints/weights.000-1.5358.hdf5\n",
      "3072/3021 [==============================] - 82s - loss: 1.7922 - acc: 0.3867 - val_loss: 1.5358 - val_acc: 0.4714\n",
      "Epoch 2/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.5833 - acc: 0.4591Epoch 00001: val_loss improved from 1.53578 to 1.53013, saving model to ./checkpoints/weights.001-1.5301.hdf5\n",
      "3072/3021 [==============================] - 65s - loss: 1.5797 - acc: 0.4619 - val_loss: 1.5301 - val_acc: 0.4714\n",
      "Epoch 3/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.6090 - acc: 0.4398Epoch 00002: val_loss improved from 1.53013 to 1.50639, saving model to ./checkpoints/weights.002-1.5064.hdf5\n",
      "3047/3021 [==============================] - 67s - loss: 1.6069 - acc: 0.4411 - val_loss: 1.5064 - val_acc: 0.4766\n",
      "Epoch 4/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.5359 - acc: 0.4584Epoch 00003: val_loss did not improve\n",
      "3072/3021 [==============================] - 66s - loss: 1.5391 - acc: 0.4580 - val_loss: 1.5418 - val_acc: 0.4466\n",
      "Epoch 5/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.5323 - acc: 0.4638Epoch 00004: val_loss improved from 1.50639 to 1.41786, saving model to ./checkpoints/weights.004-1.4179.hdf5\n",
      "3072/3021 [==============================] - 65s - loss: 1.5339 - acc: 0.4626 - val_loss: 1.4179 - val_acc: 0.5221\n",
      "Epoch 6/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.4921 - acc: 0.4844Epoch 00005: val_loss did not improve\n",
      "3047/3021 [==============================] - 66s - loss: 1.4901 - acc: 0.4851 - val_loss: 1.4232 - val_acc: 0.4922\n",
      "Epoch 7/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.4787 - acc: 0.5020Epoch 00006: val_loss improved from 1.41786 to 1.41012, saving model to ./checkpoints/weights.006-1.4101.hdf5\n",
      "3072/3021 [==============================] - 66s - loss: 1.4769 - acc: 0.5029 - val_loss: 1.4101 - val_acc: 0.5286\n",
      "Epoch 8/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.3000 - acc: 0.5648Epoch 00016: val_loss did not improve\n",
      "3072/3021 [==============================] - 66s - loss: 1.2957 - acc: 0.5677 - val_loss: 1.3306 - val_acc: 0.5443\n",
      "Epoch 18/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.3347 - acc: 0.5488Epoch 00017: val_loss did not improve\n",
      "3047/3021 [==============================] - 66s - loss: 1.3310 - acc: 0.5504 - val_loss: 1.3372 - val_acc: 0.5234\n",
      "Epoch 19/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2928 - acc: 0.5598Epoch 00018: val_loss improved from 1.30461 to 1.26809, saving model to ./checkpoints/weights.018-1.2681.hdf5\n",
      "3072/3021 [==============================] - 66s - loss: 1.2915 - acc: 0.5605 - val_loss: 1.2681 - val_acc: 0.5742\n",
      "Epoch 20/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.3017 - acc: 0.5522Epoch 00019: val_loss did not improve\n",
      "3072/3021 [==============================] - 66s - loss: 1.3029 - acc: 0.5508 - val_loss: 1.2750 - val_acc: 0.5885\n",
      "Epoch 21/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.3027 - acc: 0.5525Epoch 00020: val_loss did not improve\n",
      "3047/3021 [==============================] - 66s - loss: 1.3029 - acc: 0.5517 - val_loss: 1.2806 - val_acc: 0.5768\n",
      "Epoch 22/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2563 - acc: 0.5691Epoch 00021: val_loss improved from 1.26809 to 1.25889, saving model to ./checkpoints/weights.021-1.2589.hdf5\n",
      "3072/3021 [==============================] - 67s - loss: 1.2577 - acc: 0.5664 - val_loss: 1.2589 - val_acc: 0.5964\n",
      "Epoch 23/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2577 - acc: 0.5662Epoch 00022: val_loss improved from 1.25889 to 1.21336, saving model to ./checkpoints/weights.022-1.2134.hdf5\n",
      "3072/3021 [==============================] - 66s - loss: 1.2548 - acc: 0.5677 - val_loss: 1.2134 - val_acc: 0.6029\n",
      "Epoch 24/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.2721 - acc: 0.5686Epoch 00023: val_loss did not improve\n",
      "3047/3021 [==============================] - 66s - loss: 1.2724 - acc: 0.5671 - val_loss: 1.2374 - val_acc: 0.5794\n",
      "Epoch 25/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2293 - acc: 0.5854Epoch 00024: val_loss did not improve\n",
      "3072/3021 [==============================] - 66s - loss: 1.2300 - acc: 0.5853 - val_loss: 1.3095 - val_acc: 0.5326\n",
      "Epoch 26/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2368 - acc: 0.5665Epoch 00025: val_loss did not improve\n",
      "3072/3021 [==============================] - 66s - loss: 1.2331 - acc: 0.5684 - val_loss: 1.2492 - val_acc: 0.5820\n",
      "Epoch 27/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.2670 - acc: 0.5595Epoch 00026: val_loss did not improve\n",
      "3047/3021 [==============================] - 66s - loss: 1.2659 - acc: 0.5606 - val_loss: 1.2254 - val_acc: 0.6042\n",
      "Epoch 28/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2308 - acc: 0.5658Epoch 00027: val_loss did not improve\n",
      "3072/3021 [==============================] - 66s - loss: 1.2292 - acc: 0.5661 - val_loss: 1.2818 - val_acc: 0.5365\n",
      "Epoch 29/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2196 - acc: 0.5751Epoch 00028: val_loss improved from 1.21336 to 1.20770, saving model to ./checkpoints/weights.028-1.2077.hdf5\n",
      "3072/3021 [==============================] - 66s - loss: 1.2217 - acc: 0.5749 - val_loss: 1.2077 - val_acc: 0.5794\n",
      "Epoch 30/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 1.2124 - acc: 0.5773Epoch 00029: val_loss did not improve\n",
      "3047/3021 [==============================] - 66s - loss: 1.2105 - acc: 0.5776 - val_loss: 1.2598 - val_acc: 0.5273\n",
      "Epoch 31/300\n",
      " 640/3021 [=====>........................] - ETA: 41s - loss: 1.2084 - acc: 0.5672"
     ]
    }
   ],
   "source": [
    "#stg1 training\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional VGG16 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "Epoch 1/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.04548, saving model to ./checkpoints/weights.000-1.0455.hdf5\n",
      "3072/3021 [==============================] - 85s - loss: 1.2478 - val_loss: 1.0455\n",
      "Epoch 2/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9460Epoch 00001: val_loss improved from 1.04548 to 0.80567, saving model to ./checkpoints/weights.001-0.8057.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.9437 - val_loss: 0.8057\n",
      "Epoch 3/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.7676Epoch 00002: val_loss did not improve\n",
      "3047/3021 [==============================] - 71s - loss: 0.7734 - val_loss: 0.9774\n",
      "Epoch 4/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.6906Epoch 00003: val_loss improved from 0.80567 to 0.63040, saving model to ./checkpoints/weights.003-0.6304.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.6909 - val_loss: 0.6304\n",
      "Epoch 5/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.5838Epoch 00004: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.5813 - val_loss: 0.7017\n",
      "Epoch 6/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.5608Epoch 00005: val_loss improved from 0.63040 to 0.61503, saving model to ./checkpoints/weights.005-0.6150.hdf5\n",
      "3047/3021 [==============================] - 69s - loss: 0.5596 - val_loss: 0.6150\n",
      "Epoch 7/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.5016Epoch 00006: val_loss improved from 0.61503 to 0.53570, saving model to ./checkpoints/weights.006-0.5357.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.4975 - val_loss: 0.5357\n",
      "Epoch 8/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.4361Epoch 00007: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.4361 - val_loss: 0.5432\n",
      "Epoch 9/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.4059Epoch 00008: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.4032 - val_loss: 0.5752\n",
      "Epoch 10/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.3733Epoch 00009: val_loss improved from 0.53570 to 0.33159, saving model to ./checkpoints/weights.009-0.3316.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.3744 - val_loss: 0.3316\n",
      "Epoch 11/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.3254Epoch 00010: val_loss did not improve\n",
      "3072/3021 [==============================] - 69s - loss: 0.3243 - val_loss: 0.4274\n",
      "Epoch 12/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.3495Epoch 00011: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.3477 - val_loss: 0.4750\n",
      "Epoch 13/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.3056Epoch 00012: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.3016 - val_loss: 0.5004\n",
      "Epoch 14/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.2465Epoch 00013: val_loss improved from 0.33159 to 0.26357, saving model to ./checkpoints/weights.013-0.2636.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.2496 - val_loss: 0.2636\n",
      "Epoch 15/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.2556Epoch 00014: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.2575 - val_loss: 0.3512\n",
      "Epoch 16/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.2091Epoch 00015: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.2092 - val_loss: 0.3096\n",
      "Epoch 17/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.2256Epoch 00016: val_loss improved from 0.26357 to 0.22867, saving model to ./checkpoints/weights.016-0.2287.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.2235 - val_loss: 0.2287\n",
      "Epoch 18/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.2045Epoch 00017: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.2033 - val_loss: 0.2895\n",
      "Epoch 19/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1903Epoch 00018: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.1889 - val_loss: 0.2927\n",
      "Epoch 20/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1784Epoch 00019: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.1772 - val_loss: 0.3048\n",
      "Epoch 21/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.1967Epoch 00020: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.1949 - val_loss: 0.3690\n",
      "Epoch 22/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1538Epoch 00021: val_loss did not improve\n",
      "3072/3021 [==============================] - 71s - loss: 0.1568 - val_loss: 0.3551\n",
      "Epoch 23/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1695Epoch 00022: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.1685 - val_loss: 0.3090\n",
      "Epoch 24/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.1290Epoch 00023: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.1279 - val_loss: 0.2879\n",
      "Epoch 25/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1346Epoch 00024: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.1344 - val_loss: 0.2958\n",
      "Epoch 26/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1212Epoch 00025: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.1207 - val_loss: 0.3149\n",
      "Epoch 27/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.1540Epoch 00026: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.1538 - val_loss: 0.3790\n",
      "Epoch 28/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.1096Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 1.9999999494757503e-05.\n",
      "3072/3021 [==============================] - 70s - loss: 0.1103 - val_loss: 0.2986\n",
      "Epoch 29/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0657Epoch 00028: val_loss improved from 0.22867 to 0.20805, saving model to ./checkpoints/weights.028-0.2080.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.0655 - val_loss: 0.2080\n",
      "Epoch 30/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0542Epoch 00029: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0550 - val_loss: 0.2437\n",
      "Epoch 31/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0487Epoch 00030: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0499 - val_loss: 0.2660\n",
      "Epoch 32/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0522Epoch 00031: val_loss improved from 0.20805 to 0.20593, saving model to ./checkpoints/weights.031-0.2059.hdf5\n",
      "3072/3021 [==============================] - 71s - loss: 0.0518 - val_loss: 0.2059\n",
      "Epoch 33/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0492Epoch 00032: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0486 - val_loss: 0.2317\n",
      "Epoch 34/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0539Epoch 00033: val_loss improved from 0.20593 to 0.17129, saving model to ./checkpoints/weights.033-0.1713.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.0542 - val_loss: 0.1713\n",
      "Epoch 35/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0475Epoch 00034: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0474 - val_loss: 0.2162\n",
      "Epoch 36/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0455Epoch 00035: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0465 - val_loss: 0.2032\n",
      "Epoch 37/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0410Epoch 00036: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0404 - val_loss: 0.2118\n",
      "Epoch 38/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0423Epoch 00037: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0420 - val_loss: 0.1720\n",
      "Epoch 39/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0413Epoch 00038: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0407 - val_loss: 0.2387\n",
      "Epoch 40/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0350Epoch 00039: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0348 - val_loss: 0.2889\n",
      "Epoch 41/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0315Epoch 00040: val_loss did not improve\n",
      "3072/3021 [==============================] - 69s - loss: 0.0326 - val_loss: 0.2150\n",
      "Epoch 42/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0361Epoch 00041: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0357 - val_loss: 0.2293\n",
      "Epoch 43/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0305Epoch 00042: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0301 - val_loss: 0.1747\n",
      "Epoch 44/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0385Epoch 00043: val_loss improved from 0.17129 to 0.17055, saving model to ./checkpoints/weights.043-0.1705.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.0386 - val_loss: 0.1705\n",
      "Epoch 45/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0363Epoch 00044: val_loss did not improve\n",
      "\n",
      "Epoch 00044: reducing learning rate to 3.999999898951501e-06.\n",
      "3047/3021 [==============================] - 69s - loss: 0.0360 - val_loss: 0.2691\n",
      "Epoch 46/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0322Epoch 00045: val_loss improved from 0.17055 to 0.15367, saving model to ./checkpoints/weights.045-0.1537.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.0320 - val_loss: 0.1537\n",
      "Epoch 47/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0282Epoch 00046: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0280 - val_loss: 0.2337\n",
      "Epoch 48/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0262Epoch 00047: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0263 - val_loss: 0.2304\n",
      "Epoch 49/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0219Epoch 00048: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0217 - val_loss: 0.2491\n",
      "Epoch 50/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0351Epoch 00049: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0370 - val_loss: 0.1880\n",
      "Epoch 51/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0267Epoch 00050: val_loss improved from 0.15367 to 0.13505, saving model to ./checkpoints/weights.050-0.1350.hdf5\n",
      "3047/3021 [==============================] - 70s - loss: 0.0265 - val_loss: 0.1350\n",
      "Epoch 52/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0243Epoch 00051: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0257 - val_loss: 0.2410\n",
      "Epoch 53/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0243Epoch 00052: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0251 - val_loss: 0.2465\n",
      "Epoch 54/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0250Epoch 00053: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0262 - val_loss: 0.1908\n",
      "Epoch 55/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0237Epoch 00054: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0233 - val_loss: 0.3144\n",
      "Epoch 56/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0200Epoch 00055: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0198 - val_loss: 0.2361\n",
      "Epoch 57/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0180Epoch 00056: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0184 - val_loss: 0.1965\n",
      "Epoch 58/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0246Epoch 00057: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0246 - val_loss: 0.1858\n",
      "Epoch 59/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0248Epoch 00058: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0252 - val_loss: 0.2242\n",
      "Epoch 60/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0271Epoch 00059: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0270 - val_loss: 0.2275\n",
      "Epoch 61/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00060: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0201 - val_loss: 0.1711\n",
      "Epoch 62/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0244Epoch 00061: val_loss did not improve\n",
      "\n",
      "Epoch 00061: reducing learning rate to 7.999999979801942e-07.\n",
      "3072/3021 [==============================] - 70s - loss: 0.0242 - val_loss: 0.1970\n",
      "Epoch 63/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0244Epoch 00062: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0242 - val_loss: 0.2559\n",
      "Epoch 64/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0231Epoch 00063: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0233 - val_loss: 0.2054\n",
      "Epoch 65/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0206Epoch 00064: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0204 - val_loss: 0.2215\n",
      "Epoch 66/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0249Epoch 00065: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0249 - val_loss: 0.1982\n",
      "Epoch 67/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0198Epoch 00066: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0196 - val_loss: 0.2284\n",
      "Epoch 68/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0235Epoch 00067: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0231 - val_loss: 0.2271\n",
      "Epoch 69/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0219Epoch 00068: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0216 - val_loss: 0.1576\n",
      "Epoch 70/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0197Epoch 00069: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0193 - val_loss: 0.1911\n",
      "Epoch 71/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0241Epoch 00070: val_loss did not improve\n",
      "3021/3021 [==============================] - 69s - loss: 0.0241 - val_loss: 0.1895\n",
      "Epoch 72/300\n",
      "2970/3021 [============================>.] - ETA: 0s - loss: 0.0236Epoch 00071: val_loss did not improve\n",
      "\n",
      "Epoch 00071: reducing learning rate to 1.600000018697756e-07.\n",
      "3034/3021 [==============================] - 70s - loss: 0.0232 - val_loss: 0.2577\n",
      "Epoch 73/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0229Epoch 00072: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0232 - val_loss: 0.1452\n",
      "Epoch 74/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0228Epoch 00073: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0226 - val_loss: 0.1755\n",
      "Epoch 75/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0190Epoch 00074: val_loss improved from 0.13505 to 0.10881, saving model to ./checkpoints/weights.074-0.1088.hdf5\n",
      "3072/3021 [==============================] - 70s - loss: 0.0196 - val_loss: 0.1088\n",
      "Epoch 76/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0224Epoch 00075: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0230 - val_loss: 0.1868\n",
      "Epoch 77/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0224Epoch 00076: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0220 - val_loss: 0.1434\n",
      "Epoch 78/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0238Epoch 00077: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0235 - val_loss: 0.2376\n",
      "Epoch 79/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0172Epoch 00078: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0170 - val_loss: 0.1615\n",
      "Epoch 80/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0218Epoch 00079: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0227 - val_loss: 0.1963\n",
      "Epoch 81/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0218Epoch 00080: val_loss did not improve\n",
      "3072/3021 [==============================] - 69s - loss: 0.0228 - val_loss: 0.2624\n",
      "Epoch 82/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0210Epoch 00081: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0209 - val_loss: 0.1959\n",
      "Epoch 83/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0222Epoch 00082: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0224 - val_loss: 0.1929\n",
      "Epoch 84/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0184Epoch 00083: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0182 - val_loss: 0.1770\n",
      "Epoch 85/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0184Epoch 00084: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0187 - val_loss: 0.1540\n",
      "Epoch 86/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0222Epoch 00085: val_loss did not improve\n",
      "\n",
      "Epoch 00085: reducing learning rate to 3.199999980552093e-08.\n",
      "3047/3021 [==============================] - 70s - loss: 0.0221 - val_loss: 0.1892\n",
      "Epoch 87/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0206Epoch 00086: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0203 - val_loss: 0.1861\n",
      "Epoch 88/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0197Epoch 00087: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0199 - val_loss: 0.1536\n",
      "Epoch 89/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0202Epoch 00088: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0208 - val_loss: 0.1542\n",
      "Epoch 90/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0167Epoch 00089: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0165 - val_loss: 0.2076\n",
      "Epoch 91/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0193Epoch 00090: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0191 - val_loss: 0.2714\n",
      "Epoch 92/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0205Epoch 00091: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0201 - val_loss: 0.1680\n",
      "Epoch 93/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0217Epoch 00092: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0213 - val_loss: 0.1884\n",
      "Epoch 94/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0248Epoch 00093: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0245 - val_loss: 0.1890\n",
      "Epoch 95/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0213Epoch 00094: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0212 - val_loss: 0.2831\n",
      "Epoch 96/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0201Epoch 00095: val_loss did not improve\n",
      "\n",
      "Epoch 00095: reducing learning rate to 6.399999818995639e-09.\n",
      "3072/3021 [==============================] - 70s - loss: 0.0198 - val_loss: 0.2074\n",
      "Epoch 97/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0164Epoch 00096: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0164 - val_loss: 0.2718\n",
      "Epoch 98/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0218Epoch 00097: val_loss did not improve\n",
      "3047/3021 [==============================] - 70s - loss: 0.0214 - val_loss: 0.2625\n",
      "Epoch 99/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0208Epoch 00098: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0209 - val_loss: 0.1456\n",
      "Epoch 100/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.0216Epoch 00099: val_loss did not improve\n",
      "3072/3021 [==============================] - 70s - loss: 0.0214 - val_loss: 0.2114\n",
      "Epoch 101/300\n",
      "2983/3021 [============================>.] - ETA: 0s - loss: 0.0203Epoch 00100: val_loss did not improve\n",
      "3047/3021 [==============================] - 69s - loss: 0.0200 - val_loss: 0.1885\n",
      "Epoch 00100: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e5ab542e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stg2 training\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:14]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[14:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints file ./checkpoints/weights.028-1.2077.hdf5\n",
      "Epoch 1/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.2688 - acc: 0.5455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 1.20833, saving model to ./checkpoints/weights.000-1.2083.hdf5\n",
      "3072/3021 [==============================] - 82s - loss: 1.2664 - acc: 0.5472 - val_loss: 1.2083 - val_acc: 0.5938\n",
      "Epoch 2/300\n",
      " 832/3021 [=======>......................] - ETA: 37s - loss: 1.2594 - acc: 0.5601"
     ]
    }
   ],
   "source": [
    "#resume training\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist data_test_224_224.pickle. Loading test data from file.\n",
      "1000/1000 [==============================] - 28s    \n"
     ]
    }
   ],
   "source": [
    "#test submission\n",
    "\n",
    "import datetime\n",
    "\n",
    "if os.path.exists('../data/data_test_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_test_{}_{}.pickle. Loading test data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_test_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "    X_test = data_test['X_test']\n",
    "    test_files = data_test['test_files']\n",
    "else:\n",
    "    print ('Loading test data from original images. Generating data_test_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    test_files = [im for im in os.listdir(TEST_DIR)]\n",
    "    X_test = np.ndarray((len(test_files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(test_files): \n",
    "        X_test[i] = read_image(TEST_DIR+im)\n",
    "        if i%300 == 0: print('Processed {} of {}'.format(i, len(test_files)))\n",
    "            \n",
    "    data_test = {'X_test': X_test,'test_files': test_files }\n",
    "    \n",
    "    with open('../data/data_test_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_test, f)\n",
    "            \n",
    "X_test = X_test / 255.\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "model = load_model(files[index])\n",
    "\n",
    "test_preds = model.predict(X_test, batch_size=BatchSize, verbose=1)\n",
    "#test_preds= test_preds / np.sum(test_preds,axis=1,keepdims=True)\n",
    "\n",
    "submission = pd.DataFrame(test_preds, columns=FISH_CLASSES)\n",
    "#submission.loc[:, 'image'] = pd.Series(test_files, index=submission.index)\n",
    "submission.insert(0, 'image', test_files)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "info = modelStr + '{:.4f}'.format(min(val_losses))\n",
    "sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "submission.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###clear checkpoints folder\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "files = glob.glob('./checkpoints/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###clear logs folder\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')\n",
    "files = glob.glob('./logs/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
