{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, random, glob, pickle, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import GlobalAveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "from skimage.data import imread\n",
    "from skimage.io import imshow,imsave\n",
    "import cv2\n",
    "from skimage.util import crop\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import resize\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = '../data/train/'\n",
    "TEST_DIR = '../RFCN/JPEGImages/'\n",
    "TRAIN_CROP_DIR = '../data/train_crop/'\n",
    "TEST_CROP_DIR = '../data/test_stg1_crop/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "ROWS = 270\n",
    "COLS = 480\n",
    "BatchSize = 64\n",
    "LearningRate = 1e-4\n",
    "le = LabelEncoder()\n",
    "le.fit(FISH_CLASSES)\n",
    "le.transform(FISH_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_crop/LAG/ALB_img_01800_6_LAG.jpg\n",
      "../data/train_crop/LAG/ALB_img_03397_1_LAG.jpg\n",
      "../data/train_crop/LAG/ALB_img_03451_1_LAG.jpg\n",
      "../data/train_crop/LAG/ALB_img_03748_4_LAG.jpg\n",
      "../data/train_crop/ALB/DOL_img_07212_1_ALB.jpg\n",
      "../data/train_crop/ALB/DOL_img_07212_2_ALB.jpg\n",
      "../data/train_crop/LAG/SHARK_img_06082_0_LAG.jpg\n",
      "../data/train_crop/ALB/SHARK_img_06082_1_ALB.jpg\n",
      "../data/train_crop/ALB/SHARK_img_06082_2_ALB.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ALB': 2509,\n",
       " 'BET': 306,\n",
       " 'DOL': 126,\n",
       " 'LAG': 104,\n",
       " 'OTHER': 333,\n",
       " 'SHARK': 189,\n",
       " 'YFT': 799}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crop and cache to TRAIN_CROP_DIR by BBannotations\n",
    "if not os.path.exists(TRAIN_CROP_DIR):\n",
    "    os.mkdir(TRAIN_CROP_DIR)\n",
    "\n",
    "for c in FISH_CLASSES:\n",
    "    TRAIN_CROP_DIR_c = TRAIN_CROP_DIR + '{}/'.format(c)\n",
    "    if not os.path.exists(TRAIN_CROP_DIR_c):\n",
    "        os.mkdir(TRAIN_CROP_DIR_c)\n",
    "    files = glob.glob(TRAIN_CROP_DIR_c+'*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "        \n",
    "crop_classes=FISH_CLASSES[:]\n",
    "crop_classes.remove('NoF')\n",
    "count = {}\n",
    "\n",
    "for c in crop_classes:\n",
    "    j = json.load(open('../data/BBannotations/{}.json'.format(c), 'r'))\n",
    "    for l in j: \n",
    "        filename = l[\"filename\"]\n",
    "        head, tail = os.path.split(filename)\n",
    "        basename, file_extension = os.path.splitext(tail) \n",
    "        image = Image.open(TRAIN_DIR+c+'/'+tail)\n",
    "        for i in range(len(l[\"annotations\"])):\n",
    "            a = l[\"annotations\"][i]\n",
    "            file_crop = TRAIN_CROP_DIR + '{}/'.format(a[\"class\"])+c+'_'+basename+'_{}_'.format(i)+a[\"class\"]+'.jpg'\n",
    "            xmin = (a[\"x\"])\n",
    "            ymin = (a[\"y\"])\n",
    "            width = (a[\"width\"])\n",
    "            height = (a[\"height\"])\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "            #save cropped img\n",
    "            cropped = image.crop((max(xmin,0), max(ymin,0), xmax, ymax))\n",
    "            #cropped = image[max(ymin,0):ymax, max(xmin,0):xmax]\n",
    "            width_cropped, height_cropped = cropped.size\n",
    "            if height_cropped > width_cropped: cropped = cropped.transpose(method=2)\n",
    "            cropped.save(file_crop)\n",
    "            if a[\"class\"] != c: print(file_crop)\n",
    "    count[c] = len(os.listdir(TRAIN_CROP_DIR+c))\n",
    "\n",
    "\n",
    "    \n",
    "count['NoF'] = len(os.listdir(TRAIN_CROP_DIR+'NoF'))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3777\n",
      "3777\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n"
     ]
    }
   ],
   "source": [
    "#crop and cache to TRAIN_CROP_DIR/NoF by RFCN\n",
    "#crop images by detections_full_AGNOSTICnms.pkl\n",
    "\n",
    "RFCN_MODEL = 'resnet101_rfcn_ohem_iter_30000'\n",
    "FISH_CLASSES = ['NoF', 'ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "import pickle \n",
    "with open('../data/RFCN_detections/detections_full_AGNOSTICnms_'+RFCN_MODEL+'.pkl','rb') as f:\n",
    "    detections_full_AGNOSTICnms = pickle.load(f, encoding='latin1') \n",
    "    \n",
    "CONF_THRESH = 0.999\n",
    "outputs = []\n",
    "\n",
    "for im in range(len(detections_full_AGNOSTICnms)):\n",
    "    outputs_im = []\n",
    "    detects_im = detections_full_AGNOSTICnms[im]\n",
    "    for i in range(len(detects_im)):\n",
    "        if np.max(detects_im[i,4]) >= CONF_THRESH:\n",
    "            outputs_im.append(detects_im[i,:]) \n",
    "    outputs_im = np.asarray(outputs_im)\n",
    "    outputs_im[np.random.randint(outputs_im.shape[0], size=1), :]\n",
    "    outputs.append(outputs_im[np.random.randint(outputs_im.shape[0], size=1), :])\n",
    "\n",
    "train_outputs = outputs[1000:] \n",
    "\n",
    "with open(\"../RFCN/ImageSets/Main/test.txt\",\"r\") as f:\n",
    "    ims = f.readlines()\n",
    "train_files = [im[:-1]+'.jpg' for im in ims][1000:]\n",
    "\n",
    "for i in range(len(train_outputs)):\n",
    "    filename = train_files[i]\n",
    "    print(i,)\n",
    "    bboxes = train_outputs[i]\n",
    "    basename, file_extension = os.path.splitext(filename) \n",
    "    image = Image.open(TEST_DIR+filename)\n",
    "    for j in range(len(bboxes)):\n",
    "        bbox = bboxes[j]\n",
    "        xmin = bbox[0]\n",
    "        ymin = bbox[1]\n",
    "        xmax = bbox[2]\n",
    "        ymax = bbox[3]\n",
    "        file_crop = TRAIN_CROP_DIR+'NoF/'+basename+'_{}_'.format(j)+'.jpg'\n",
    "        cropped = image.crop((xmin, ymin, xmax, ymax))\n",
    "        width_cropped, height_cropped = cropped.size\n",
    "        if height_cropped > width_cropped: cropped = cropped.transpose(method=2)\n",
    "        cropped.save(file_crop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbe0032a0b8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAE5CAYAAABmlq8RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJztvXuUJFd95/mNiMysru5W04CQ+qVuGiNf4bEXPGCw1ou9\nu/LCwTu28XjHgBhjg2U9kKBXgC1YhBDdwGLZgB5IjGzswQ8YLK/nHAzjQbbxYXYQXjEGgcaGvlJL\nXVXdXVXdejRqqR9VmRGxf0RmVd7fvZ33RlRkZZb6+zmnTlRE3Lj35o3IiF/+7jd+vyjPcxBCCCGE\nED/xqDtACCGEELJWoOFECCGEEBIIDSdCCCGEkEBoOBFCCCGEBELDiRBCCCEkEBpOhBBCCCGB0HAi\nhBBCCAmEhhMhhBBCSCA0nAghhBBCAqHhRAghhBASSGNYFSulrgXwHgBbAHwXwDu01v9tWO0RQggh\nhAyboXiclFJvAPBxAB8E8OMoDKd7lVLnD6M9QgghhJDVYFhTddcDuFtr/Sda6/0ArgZwCsDbhtQe\nIYQQQsjQqX2qTinVBPByAB/tbdNa50qpvwNwaUgdeRTlAIDdu4GHHwYuvhg4eLDWfuZRQJncXI+E\nnZmJOqLIX6msU/YjFwUauW3bpmgZ60cmJ4z1w5MbjPXHJyYBAOt3XoT/7b6/wd/+1GtwbO4Jo8yp\nqGmsnxGXxumW2Y+nm/ZnPWF2A2daZj+zhtlGK19n1THRToz1RsdsJ4vOWMf48J6XOBm830EG4Hnb\nLsT/9Zefxkd/+Ro8OXusdB3WNeg41wP7kNifS14/3v2yzdh/DcvxzAPGTx4jv0t5BDx/6wvwgf/w\nCex707vw5PzjA493kWWZt4xEjof8Tsv9cnRD7iOyTqsP/iq8dUgSJDh/6wvw4c99HDe++d14fO6x\nchXU0AfAMX4V6rDqrHBMGpe/NiQv2HoBfvdPPonfesv1eGyu/PfdRenvqwPfd0Puj/PF0nVadcTx\nwP0AEHvO1PlbLsC+f38bPvDWPXh8/ph165FtuLb5+pFE/vHr5OYz5ba/+gvvVToMjdP5ABIAR8X2\nowBUUA27dwObNwOXXIIoSZD/5E8W63US8gW2xlycSFlHwM3dqlMaX74HGwBAGCDrzPUNXUOpR7tV\nWDQbX/wiREmC5778pehMzRplJsWlsAjzYbjQNPuxsWF/1k2mnYTFptmvLDHbaObiAACtjtlukpr7\ns8j/pZdEvpPt+IL6yABcsGsH4iTBrh9VmDxvY+k6rG6VNZwcxX23CdtwknerAMMJ0nDy9zvEcLpw\n5zbESYwX/ouLsX7TRlHeTxbwkJH4Hux1GE6+MkGGU0CZfhIk2LJzK+Ikxot+5MVYv3GD/yBfH8bE\ncKpCHYbT1ouK6/OHXvJirN+4voZeBRhGIReHZ0xtw6ntr1I+y2QdkTRYSncLW7rjufslF2Ny4wbb\ncHJUGlmGk7yvyH76BzDtMyQPPxrmoIlCLNoyKKW2AjgC4FKt9f19238HwE9rrb1epzxN8ygp7wUg\nhBBCCKnCnl/4NyPzOD0OIAVwodh+IYD5oBouvhh5z+P0+c8jv/xyYP/+Wjs5rlN1skDs8EKcik1P\nzuMT5vqxdeYU2PGJ4tfRph96Ef7HO34P33jHe3B06ohRZkFcCm3R7qLwOJ12eJyeEWU6TdP4zWO/\nx6mZyqk6c38aj5fH6Vc//G786Y0fx7HpI95jHB0zeZZ4nEK+B/K7hDjCBTu34tduuhZ/vPdOHJuZ\nE+X90OO0TM/jdMVN1+Ize+/EvBjPKtDjtA1vv/EduOvDd2Du0Kz/AAf2969keQfe75t8Tjmuplhs\ns6fqBn/Hq3qcrnjfO/GZ//t2zB+atepIavA4RQHfrqixPK3RaISZRLUbTlrrtlLqWwAuA/BXAKCU\nirrrtwdVIvVM+/cDDzxQaz/HdarO/mLZnrdUGE5nhMbp6UnTjXx80pz2OPHIo3hivznGp8X036J4\niC+0zH4849I4CR3UYsu8vPIkQOMkpuqabbOdTjw+Gqcex6YP44gur8F7tmqcqhpOPY7NzOHwQ1Ol\n66TGaZmkb6p9fmYOMw9Plaughj4Azy6NU4+5Q7OYrjie1niMQOMUIbXK+Awnn5aoisapx/yhWRw6\ncNDWJznuRWU1TvJzueg3nEIZVhynTwD4bNeA+iaKt+zWA/jskNojhBBCCBk6QzGctNb3dGM27UUx\nRfcdAK/VWq/81Q5CCCGEkBExtMjhWuu7ANw1rPrPFZ5yzCn+QMwsPSlmPX4gptF6oQNaXV3SyUaE\nZxpmJWeED70j2k0ToV8KmZYUyBccYocnNxGe1USUEZKnSsjpgtWQXYxK27EahEyjDaPOYbRLyDAp\nOzVX5eUtaypd1OH62uSR+TwIeRvNRy50UaFTd6Xa8IxPFgW88ZuVn6pjrjpCCCGEkEBoOBFCCCGE\nBELDiRBCCCEkEBpOhBBCCCGBDE0cTurheMu2bZ8QqurHGub6cRGI8kQ3BtO67vaTzRhPiTJtK5qi\nuDREDKbMoezOLUGhjKlhEueOGBsy6KgMHlMhoPxKhdlVIsA8m8XgElt8eg59+DElz3Pk3S9TjryS\nyNiqc8U1EL9wu8JLN2XjOIXERBN3axlYMsHwv/O547Ud33hVuc7zCvHf6HEihBBCCAmEhhMhhBBC\nSCA0nAghhBBCAqHGacx5qmmfouO5GbBLBsCUx5xoFcl0NzQKndLJRhNPizxyaSbyjYn1RORF6ziD\nqJWb544yez46tuaoy81Zu7RFZWff68tqtTKq5AYbB1w6A68WIV/eluf1aHIIGTdkbrVR5KqDQ1ta\ntg4Z3NJ1n7aOGUKo4To0T7lLa+uBHidCCCGEkEBoOBFCCCGEBELDiRBCCCEkEGqcxpyFxD5FJzvm\nvO0zYr75VMM85tREoW0609U1nWk1cEok+c1Sc30ibpmNirn5DHZixDwanCxRhnnKIntuOc/lvLhI\nejkE3U8dmqY6+rVWNU2ZiIPi0ktYWgT5m01onKrUSZbp14nVpRmrUgNjepn4xmMYcZ1s3U/5OurA\nqrO3HkXIoxiRvNc7Omprq6R+q/xVutBZLH0MPU6EEEIIIYHQcCKEEEIICYSGEyGEEEJIINQ4jTu5\nnZytk6Vi3bR/O7F5TNrNM5fFjaVlR+aey8xLIYnN/VLhILVH7m0y9gcGrgNAGpvalngIwh9ZZZUW\n8qjvwIAKxiU21DCQWoSwOE5yRKKlnFF5llXS5FBPQ8adkO9Kmf1AhdhFkf1Mse/cZePn2XXGVu5S\nXx0yP15IDCaZ3NQ/FrLeqEICVHqcCCGEEEICoeFECCGEEBIIDSdCCCGEkEBoOBFCCCGEBEJx+Jgz\n0bFt21ZqCuASEXcyEULvaOk0J0vLTAriYimQE0I9oeSOAkR3UnIo5cAucXgnMUuJOIh24MQAVqov\nrxLc8tksBpdUCq7ouFR61eR5tTopDifjzjDE4WUJ+Zrk1n3WPCgOeiHGrCMueVe0g13WlNRXfJbJ\nyclS/QLocSKEEEIICYaGEyGEEEJIIDScCCGEEEICocZpANZcsGf6NGw+evDksJyzfX5jg1XmpDht\nx0XwsWc6Qo/ULvoVdZaXDaFxymEGvIxk4M3UFFLFjvlna5MIcpiK9UUrCCKQCFu+IaVYJccPqBDg\nUkzgn/V4I0ll4DEDkHX4kIlw6yBII2DpDMr//rK0CSXbdJGmniTTAeNrBQL0HePYPa7Jh1eqp8kD\nhC1yvOQ1GqIX9I/52tWy+b6z8rPHsf+75Uvqa5f3Vumv09ofcN8Qz6lOtrzsZEDsSBzvRQSDluPV\naNgmjtyWoVO6WXqcCCGEEEICoeFECCGEEBIIDSdCCCGEkECocRpzNjmmXzel5lzxBqFpmhDHNLr7\nk3R5mYjEwJmYok5k/KRc2tgOfZLY5AhBZZC65trlNlFHq4Ksxxv7o0LS3yxa1sNUVbRUiQ9Fzk6V\nGC+M/UTWEnVcrx3H9yD2fHd8667bsjfJb1/gtjzPkUXl4z5FiUhOL/RLraZt4jSbpp63grKKHidC\nCCGEkFBKe5yUUq8G8FsAXg5gK4DXa63/SpTZC+AKAJsB3AfgGq31gZV3lxBCCCFkdFTxOG0A8B0A\nb4djlkIpdQOA6wBcCeCVAE4CuFcp1VpBPwkhhBBCRk5pj5PW+isAvgIASinXhOseAPu01l/ulnkL\ngKMAXg/gnupdPTfZ6JiA3SDEMRvE3HBTTA0naWHfJtnyUuqRIOI6JWI9zoRwyhG7JxVz2pFnjtql\n8UlFtVJ75dM4OWPR+OKaeOoIiV/jg3qm4ePLAxakD/HGwCkviKtDRkUp1rOTOjRLZetwxZKyYsiJ\n705S4btkx7aTwtnlOHiu+jqOe32UD47bZN3/rRys9rYkatplPNSqcVJK7QawBcBXe9u01icA3A/g\n0jrbIoQQQghZbep+q24Lium7o2L70e6+MHbvBjZvBi65pFjvLeukUmhnYWdW+rFQ8hdrcqG1aUJE\n3N40YVbygvM2mlVsPK/Yvnvn0vJU08wIncGcSW2KzxpbUZntnwNt4cZabJjHpJE/irB8e08WmfS9\nqufC+4bI4DrPFtH7gl3b+5aDT2TYm3clfznWEaFanIQqnjFXFnMfkfjNlgO4cNc2oLus8kPcNx6R\nY3ytH9oyarwnYrKrxVxsrSO+e1Z2PDJgy85iPHvLlUYOlxH1nXg8F0GRw33fgwoXRxqv/CxsvWib\nsaxCLl3o3o/qjwruHS9BJ7enMXweJPkND2nTrtM8B77xdL4FK/ou790yKvjEhK0QsqKJR8uzKUcO\nzjj7YvdjBTdfpVSGPnG4UupSAF8HsE1rfbSv3J8DyLTWbwqpN0/TPEocLjZCCCGEkCHwW2/4Dfzu\nn/+h1yqs2+M0j8KGvhCm1+lCAA8E13Lxxci7Hqfo859HfvnlwP79tXa0kuZEeENK/woEIH9i+PrR\njG1H3WPi19MjE2a/pjasN8tvKPLdvWD3Trzxlpvxhd++GTNHnjDK+DxOSUf+UrIDTMlfdYvCA9VO\nxK/PkDgd4rfOuo5v0F1eBd+vNnFOAjVNF+zagV/b+2788U0fx7GpI2Ydla4N2wszkGehx+lte9+J\nP7rpdhybmS1dZyq0G9Z5Pwc9Tld+8Dr8/oc+hfmZWXqcVsjWi7bh7Te+A3d9+A7MHSp/fQL+Mfbn\nmfN7nHx1uDxOfgbXWcXNseOF2/Gb770Wf/CxOzF/eM7SXrlyT+bp4LxyMkZTy+FxajbMMv2rIbkB\ngZoNJ631QaXUPIDLADwIAEqpTQBeBeDO4IoOHjTX9+8HHgi3u4JYM1N1O61N7di8oJ5eZ162j20y\np+pmN20y9x+cweGD88a2FBPG+kQmBHRtKeyzL+BOYvZroTHYkMqjAMNJjPn6tufCdt1YvIaTeIiX\nFIMfmz6Mw9q8ZteO4STarDJFVofh1Nfu0elZHH54qnSd8kYbNM0hDaGyhpNjvOTD0Wf0hJzF0oZT\n31DMz8xi5qGpkST5tQT7QbbXeBpOPeYOzWK6wvUJDMlwKpnkt52VN5x8dVYxnBqN4h4wf3gOMwem\nLMOp07GfMXnaHlhnq2UaSuvWrfOWaVZ4379KHKcNAF6M5Uf+i5RSLwXwpNb6EIBbAdyolDoAYArA\nPgCHAXyxfPcIIYQQQsaHKm/VvQLFtNu3UPxY+jiAbwP4EABorW8BcAeAu1G8TTcJ4HVa68U6OkwI\nIYQQMiqqxHH6L/AYXFrrmwHcXK1LhBBCCCHjCZP8jju57aiLRQCvqCHmn4XWqDe/n/Ut7Tl/c+49\nFa/Nyjns2BVn0pMoOBHmtnum3RMaYOBet2TMryvwVBpAHQEua1AskQGEvEFcVjbGc0aGSVn9Ugih\nAuhBWP2qoLfsPWKyHMjyyBvAFgAy8XxIxENHfrbE8Xa+DEcQV7CCmOSXEEIIISQQGk6EEEIIIYHQ\ncCKEEEIICYQapzGnndtxK9pSYNQw7d+OCDTZW+/Ey0uZuURqdKK2jLlkRQq0+pWIOBxZLPeLAxxz\n7XJW25eA1xc3xlVGYsXICdHCYDnFR5bnQfoZq44K7fbDvK8mdWg3JD7dhTNtUECQzDL7Q8sYfeDV\nMZbUoVEqW6fcHw+hD660VLHnftaLu5amaRGzKSAwp9QsSb1Sc8KM2zQ5aaYWA+zYTinKv/BPjxMh\nhBBCSCA0nAghhBBCAqHhRAghhBASCDVOY87p7Iy1bTE2523bDbemaWm9G7Mp7Wqc0jhHKo6RmUjb\nublhnYjkJGM2AXYOUFlGrrtiQZXFp3ly4dM0DUOHUEecJzKYKuctJHZMP1mAxklqrahxIqHUce/x\n1ZEH+EuiWlJTDybrBnLKsrybp05qsRz9Ep/Np3mKG3YiOlkmdeTE80GPEyGEEEJIIDScCCGEEEIC\noeFECCGEEBIINU5jzqIjtkUayVx05v5MTA731rNoeZnK+WOpTxKqCksXFIA8RFrpFUIf1YKtixpN\nP0i9eLUdVWJt+eI4hRzjayOkHwFlyLOPYegtXdjfjZVfcfLJ5YvrVAXZ70zEEuzFiupnsWOW6VDj\nRAghhBAyPGg4EUIIIYQEQsOJEEIIISQQGk6EEEIIIYGMpTh8oRvgMYrXYQJAinUAloM+Js7gXFLg\nZYrCFsQnPdO0a5C1rhO6svVtISoWiW4jTDj6ZQboml1nHnNovVn6MVHFU41dVo0LiVnH0y1zfbFh\ntjmZFx9+XZ4sLSc7ZmDNRSE4z9ebA/aMENk1LHU50BBZfWMxPonU6Tl0j1LobjfjEwDb26xggDIw\np09UfJYgh3n365OjsfR/jypi+rIxC0MC2ZVts0q/QxLfWs1GIok0gE53WyfK0I7KB+DL4vIiTwtH\n8urS/SgprA0JVpmU1dXmOdANMIis+N/bii9ZbEAffJ/dVYe8Xrwi/koi48RfxEeWLC+zavX5ElHn\nWbkXCwDY502sW/c31/0sEvfuePDnk+cozexk9FKo3c7MB0CULC+jRmQLvR0XSyKeffGE+Kwi3mUn\nsvuVi2dZ3mGSX0IIIYSQoUHDiRBCCCEkEBpOhBBCCCGBjKXGySaDqUAqr3/wJZwtNvpqkXamP5mn\nnGFdFHPJC2IquS2aaEe2bSvLpKJMKubRM9dSajlKaltC9A6joI6AceOakNfq15ieA0JIGFWCsq4U\n1z1SbvPdRkP6LTVOZwtOmaYpOp2Opf9qNGydVavVEuumWFkm8HUhA16mi9Q4EUIIIYQMDRpOhBBC\nCCGB0HAihBBCCAlkTDVOLlWOT+M0eM5VWogujY49oyoDjAxOW+uKlPKM0B+dEHEoTor1Uw2zjtOx\nXeeimAtekLoo0Y9e1Ir+0bTmqK0APub+RAy5y+KOc7OQjHOVy9g8jnl0a4itEEzmWaqiaRpG4sxK\ncZs8jKvWSrJW+klM6jhvw7ju60De3uv4zveqiKLq9fm0Qb79rnbL6qQih25W6os84aaC2rRiPYn4\nSVLj1GyaeqUksZ/IExMTYt08Rn4OVwJf2Y9224715IMeJ0IIIYSQQGg4EUIIIYQEQsOJEEIIISSQ\n8dQ49bQwS8t8+X/gLLmKBsd2kpqmpqO4jI9k7Zfzy0LDc9phhx4XGia5/pSY1316wjwlJx3Kq7aY\nxz0j6uyIPENZd07bXIo5bdFGJPPOCbGRU+Nk5USylAbGWh7b59GTdmkomiap9RiGBiqk3VFQKTdd\nDfGkGIJquOTR8nnq/3/U1NGPKtdOHddb3resWt+KNU6ubb77m6gzqSGOU8g90sphWVK/5crrJ3VP\nskyVfnUymUTVDz1OhBBCCCGBlPI4KaXeB+CXAFwC4DSAbwC4QWv9kCi3F8AVADYDuA/ANVrrA7X0\nmBBCCCFkRJT1OL0awB0AXgXgZwE0AfyNUmqyV0ApdQOA6wBcCeCVAE4CuFcp1bKrI4QQQghZO5Ty\nOGmtf65/XSn16wCOAXg5gK93N+8BsE9r/eVumbcAOArg9QDuCWupJ0A626xyQK46MdWZiOnV1BXH\nSVYr4l0sSi1Rbg7fUw490uMN85jHmuYxT7Skxsm0L0/ntr2Zis/WTswNMs5T2j3NWbd/xVLMaQu9\nlhwvue62uD0z/yH6BhnrqSSrFdcpExoSH+OiMRlV3J3BajeyFqhFD1eBZ5MebqW56YaV287WI5U7\n3nUPlfojqU/qrSdJgkaj4dQ0SXz57yQh41Xl/r9SjdNmFNf1kwCglNoNYAuAr/YKaK1PALgfwKUr\nbIsQQgghZKRUNpyUUhGAWwF8XWv9ve7mLSgMqaOi+NHuPkIIIYSQNctKwhHcBeBHAPxUTX1ZItq9\nG9i8GZFSxYZLlFkgLz9VZ+22Z9VsK1IUinLxKmRuTrM1HXbo+oZ5zOb15jHpRvMUrF9n7l/In2vV\nKafqOiItS0e4PNvdz3HhCy9aWmbimExMMybCL5+Iz+4YPr9PXU4xulKuxMIVK/bHaRODiKpMAjnS\n2vjIY+DCXdsBLC8Hli/dQgjlf/esbCK0i4zKEZJ+QVbhcI9v6Y7jll3bq02fxuVfK7aoZU617Hmp\nf+IyR4QtO7cBwNJyxXWu4am6PHbesUqxdedWY1mpH9kQ7gQlQwkkLfseKu+bvlkzOUWWZnZqE5nu\nJBNltu3cbizlVF5rwjZPJibWGevN5uDzmqX2PSEV4QcWF04v/X/okamB9fWIqsyZKqU+BeDnAbxa\naz3Tt303gEcAvExr/WDf9q8BeEBrfX1I/Xma5pEjTw0hhBBCyDC46uffiLu/9AWvuV/a49Q1mn4R\nwM/0G00AoLU+qJSaB3AZgAe75TeheAvvztA2Fi+5GNHmzYjUJWj+2efRefPlgN6/tN8ScQOlf7RJ\nrw0ApMLKll6YDoRwOzIt9x84zPR54XE6JjxOxzeaFvQZ4XFq4zyrzlz8opWizVSI2nu67wt3XYS3\n7X0//uimj2Bu+ohRJhbi8EhU2pC/SHL7s8p4l/LXZiaE365zIKuVvxeSzmCPk4uVBrh0fFQAhafp\nbXuvxx/d9EkcFeNp1VGqxTCy1QrD5hkuS1gaUuVZPE6/+aE9+IMP3oa5Q7MlOtirlB6nHj2P01U3\nvwN333wH5mfKj2ct3qIqXqqVN2uRuaYYSrJ151Zc+4Frcee+OzE3M1etEuu7svJPa3nZPR6opOl4\n7MtjPP2Se/PclUxXPqjN9a0XbcO1738n7vzI7Zg7NItYeP4bDm9SU/S9IZ6vciwyx+yUJTDv84zF\ncZhJVDaO010A3gTgFwCcVEpd2N31lNb6TPf/WwHcqJQ6AGAKwD4AhwF8MbihgwfNE6P3Aw88sLxe\ng+HkKi8fkNJwyiIzM3NbrJ92GE4nxIl+fKNpfD32nPXG+qkNZp2L2Gz3U9yYpQEiDSc5LXd0+hAO\n6UeNbdIQikXkcGk4JVl5wymN/YaTrFaWSdrlo1qs3HAaXP7o9BEc0gdL9aEOVstw8vW9LsOpx/z0\nEcwcmAqoRVZq37xLczYruRTjYDgt92F+ZhYzD02VrqOONzDHxnCqYaqux9zMHKYfnqp0bNlo2iHY\nUb8Hrzun6qxjykU4z5yGk/lDJj+LxGbu0CymDxy03qprtuxz1mqZ9385VSc/h+utu0xM1aXttrNf\ngyj7Db8awCYAXwMw2/f3K70CWutbUMR6uhvF23STAF6ntV4s3TtCCCGEkDGibBynIENLa30zgJsr\n9IcQQgghZGwZyyS/S7lfz+YtrMGFLAM6AvbLeh0R8HIhEQl4xfpxx/zoE8Ld+KRI6vuDltA4Nc31\nDP6pqTwabM/2glvm3dOdo4EEcm5YBCsTddj5eG0X6DCmozwe46A2656aC6qjhrGo5Q24koxqaiWL\nlqeGqk4R1TLFM4rInCOK8DiMYKjjMjVntVFH4MheHXleub4VJ/mtkMQ2ZL/c5puqC+lXoyGfh7KN\n5aXreFc/rbf5PLLGkKm60tE+wSS/hBBCCCHB0HAihBBCCAmEhhMhhBBCSCBjqXGSs52xzPE7rHbF\nq8iZGJ5FoWE6KfRLP4jt1zyfFGWOCw3T04mpYWo3zHAEuaNOqTeS09EyVMBSD3qxTKIEUSSjoIs2\n5EkIiQ7te229ZPkqbVRhVJqmUWiYJKPUNJHhIZNQD2u8y14/o7p20hpa7tWRIq9eny/qvvez2e3K\nQ+zQAub+OOReLgS/ZUMeAHaSX1mkTzKGPM8r6bd86zIkQrHNDJ0wiiS/hBBCCCHnDDScCCGEEEIC\noeFECCGEEBLIeGqcetOr3WUuNE6ueXWfBWhNYzqmeRNRS0fk2sk3mnqjdL2ZR+7oMyesOtvP22Ks\nn2qINiY2GOsLudlG0rDjOOWpiIchMm7LPEPx0lxytLS04nZIwY1n3lfm9XMhhzhEmyD1WlJ+NAyN\nhK9bZ9Nx+MKNGW0GlKmDoWjAaqij7JhbfaAmqhR5X6yhvGLcoaAx98VZK91qwH2iSqUVdCyrgU9f\nE6K/KavRCY2Z1I/UKyWJ0MgGZLSRuqlenrlGI0Gj0QjSJ9l1DtZaueM4rfxuTI8TIYQQQkggNJwI\nIYQQQgKh4UQIIYQQEggNJ0IIIYSQQMZTHF7hGClkLJmj8Cx1mnZlOzGF22cmzPXTbTO4JQCcmjAD\nWrYjc8jzxnpjPRIqu8wxGvKz2ro8/4ePMmkzCyG83BsNXnf1C7nHLnd0M5JiP7HfJxd09ausLnRc\nE/SuhkB6VAEKcxGwsVJA0fHU/46GCieyjutrKGJwUishomvfHcwObmnf6+0gmWaZXh1xHCNJEku0\nnTuuJikgz7LBV5zrs8okv42GHWTaBz1OhBBCCCGB0HAihBBCCAmEhhMhhBBCSCBjqXFaVrZExloP\n5wytR/djT6PbWzJhR7bF9GlHzOsuNsxWFpv2XGmnJZL25uYxSWTqoloiqe8i2ladkQxeKUZE6oSS\n7jxw0j0syf06Mp+myaWHkOMnG4mlfsmhgZLJhiXtIZj646JpGkWQx1FqmsjwqEMzFtROyfJ16Jl4\n7awMVwBIO1mu+UyRAS/lugxE6SpjJRvu6nnjOEGSJJYmKg3oZyqDVHuS/gL252eSX0IIIYSQIULD\niRBCCCFvzxwfAAAgAElEQVQkEBpOhBBCCCGBrAmNU7Fcnod0xXewNDli/0SQCMW0I1Mxb7sgkhSe\nEf04IzPSAuhEQm0lNE0NmdQ3NZP6pg6Nk9QBJSImU5JJzVP3n76lT0skp6ylrkzqvVzH2For85iG\nFUvK1j1Z/YxDYpAMnwzL11jIpTUuuoyhaJoCKrWujRr6QVaXUcVoqiW+VI11DFMzZhHSTsm+uGIc\n+rRBUhck4yMljhMtj4nF87G3HseRFRcKAHKHX8eK9eS5KF36Jam9sjLJB0CPEyGEEEJIIDScCCGE\nEEICoeFECCGEEBLImGqcYrFM0B+ZKYtsnUsqJm7ltGWKwXobwJ63zsX86KKYT32m0zHWT6X2hGtH\ntits1VYmTkHHLO9If4dYiGoSsd6Q+3N7GQllTirHS6x35LrD5M6iwWMcWXa6K46TmAe3ztPKNU5l\n9TbDyDtXB3XolUIoq1MZhv6D+cxWn9W6vqx2ea6Hikv3I7dJ7VBZzZMLqXHq1ZnnuTPekkv3JHEd\n56tDfvwq93d6nAghhBBCAqHhRAghhBASCA0nQgghhJBAxlzj1B/Hqc/Gyx05bIRSJfNoeOKAXHVx\n04yp1BZ1nm6bMZbOtO2YS4mYsG9E5pC3UrPNbMGcsxVdAABElqbJ3JDIOE/d9bhvKeMjyfGx4jgJ\nE9ulcbLs8EjElxLryGVGQSCyYlK52jk7Tn3EKHLArVGdxqi0RJnIrbam89+VFQcNo98VBErMW0hC\nsXPG2RonWcbSFnWfW1mWIU1TS4/k0mLJMlU0TlJrlXXKX/n0OBFCCCGEBFLK46SUuhrANQBe2N30\nzwD2aq2/0ldmL4ArAGwGcB+Aa7TWB2rpLSGEEELICCnrcToE4AYA/xLAywH8PYAvKqVeAgBKqRsA\nXAfgSgCvBHASwL1KKceEEyGEEELI2qKUx0lr/Z/EphuVUtcA+EkA3wewB8A+rfWXAUAp9RYARwG8\nHsA9K+8uIYQQQsjoqCwOV0rFAH4FwHoA31BK7QawBcBXe2W01ieUUvcDuBSlDCcZADNGv3MslyJj\nAJmVuVAklBUKRJeIUYoUmxOmo0yKxxcWzQCYCyIgJgBMCuVnMzaT+jYzUyDdaZsiOxnMErDz3Eqx\neCJFeV0xfS/opQx+6UKOjx0gM0DRKc6TFPC7HZ6eJL+CoQRbrL/KSng/26giFAoo7l2bjMnlQ1YZ\nGbzStc0Wdpf/ktvtuBMJ9wJgyjaliNvVD9mGb71bi+iHo4iH0oaTUupHAfwDgHUAngbwS1prrZS6\nFMXIHBWHHEVhUBFCCCGErGmqeJz2A3gpgOcA+D8A/IlS6qdr7dXuFwKbNwOX/HCx3lt2iaTLBUAs\n38GH9LrIdyFd1qzpDWqsX2+sb2iYw/V88arjRW3b4zSxfbfo6HnGagsbjPX0tPnZzpx32u5nJtfN\nY5JMfPauD+WCF+5YWsrRkuEF2g2Rbka4fjoOr5/tLTK9aS2Rt6XVscMRyPAMTfFZN06cMdZDvB2+\nX0tyb6jHacuu7cZyYKUV8Huc6n8pttLr5FV8FzLFQwRs3VmM49ad2yv9ws1rSMdTT2yAsuelfpdd\nngNbd24DsLy0ytTeajXv42p4vlLn/aoc27rjuO0s4zkMQr4Hpb8rmT3imQzxI9ZlG0li3rvjJKQP\nZrtbdmw1llY4goBTlotnn/wcIell0r5DZh6Z8jcKIPLFQfChlPpbAAcA3ALgEQAv01o/2Lf/awAe\n0FpfH1pnnqZ5lNgPVUIIIYSQYfAb/+rf4g+//GdeK7COAJgxgAmt9UGl1DyAywA8CABKqU0AXgXg\nzjIV5hf/GPKuxyn+/GeRXf5rwP6HlvYvSpcLgLZwTUTCIGyJn0KJ46d1KjxOC5s2GeuPCktVi+P1\noh0Ac8NFP2T2K3mOsT6RTxrr7VOm1+r0eaaHBQBiYVUnqfSund3j9OaP/DY+9/5bMD912Cjj8zgt\nCI9eO8DjlAiP00R7sAcKACY6g7VpT62zPXA+hulxumLv9fjMTZ/E/MyR0v3ycS56nK7+4B78uw/d\nhvlDsxX6QY9Tj57H6ZoPvhOf/tDtmJuxx7MOT08d+rZheJwsdU1NHqd3fuA63L7vU5h1jOcwGIbH\nKXNocaX/JIoGa5wSMfsSkI8X8kxv37UNV91wLe7+nTsxf3guyItlJxvuiHX/3VsOV47yTpqycZw+\nCuA/A5gBcB6ANwP4GQCv6Ra5FcWbdgcATAHYB+AwgC+W6tXBKXN9/0PAA99ZWs0dhlPWMm+a0njI\n5RPB8YTII1MMnj73ecb6SSFWe1wcP3Nm0apz00lxATbMOifzjcb64tOm8XXyuaesOmM5NScMp1hc\nPFIMfmzqMI7oR4xtbXHhLzTNz3pGGE6Lsetik1G/zctrUhhO62Sjjm1NcZ6eXH/S0e5gvIaTFByW\nrH9++ghmHj5Y8ig/a8Vwyio8+qwx71udmzmCmYenSteZR/YDoTw1jGnp8zIMw2m5zrmZWUw/NGWX\nqaGdOiLNr8YLHnUYTj1mZ2YxVeH6rMIwDKfUISmxxeBSqC1+0ErDKcj2MOtMuobR/OE5zDwybRlO\nScNvOKWp+bwcS8MJwAUA/hjAVgBPofAsvUZr/fcAoLW+RSm1HsDdKAJg/lcAr9Na2xYFIYQQQsga\no2wcpysCytwM4OaK/SGEEEIIGVvGMslvu1XoWKLmGbQAdBoLyJvLWp84twORb2iLN+Kkyy4WTi+H\nd3OhYbownxBuwWSb+Xbfgf1zZhNbzP0AcKb5fLOMmLddXDhmrG98rpx2s0+RdG13RJHc8n0XrsiF\nRry0XBBuUelyT4RcZIPQHpnvArr7BUiXsHlOZL8B4ITninQlZ14paYUXJKq89SVZ6RRFSDyusn1Y\nteSV8noDEHdd+TFyJFUmktLyt7OVnoNqmjD/lpUSJx3EcfH9i+MO4sTWX1q9sL4H/oSq3kmOkGlL\nq9r6v+Put4DPjuu89mavonz5/0HU8YZh0MtbJe9fufUGuut+JjWIUs4gHhCO82y/JSfjJ0VLy96f\n0YbMLA8gyoUkR8RBjEU/XEl+5ZTgQmbriH0wyS8hhBBCSCA0nAghhBBCAqHhRAghhBASyFhqnKoQ\nWXOsMhKqPMCuQ85Jy/llOTe6KOI2tVq29qpjvR5p9rMl6owys85xzQM2rv0ihBAy/vTnqsuyLEg3\nKkM7yWOkpkk+s11l4gr+I3qcCCGEEEICoeFECCGEEBIIDSdCCCGEkEBoOBFCCCGEBDKW4vD+QGNA\nV8fdr9MeRlZIB0nLDKo5uc4M+5iKnHHnrV9v1XFCBvUSYvGGDESZLohOBHV1TbLy8I2jI4+w/ILB\nOSaUX6vnrY4XGurIzTYM5Gcb026SZwm+F6mc2zIZAHNZHN4TiPvbFQGiPeXDcteV/7bQ40QIIYQQ\nEggNJ0IIIYSQQGg4EUIIIYQEMpYaJ2nN9ctJAARqnFy19NdhVyL1CxMTk8b6maY5XJGoc3LSLA8A\nJ0RuTTnnGjfNfnYWRGLcZMKqsw7KajVq0YesvIpaKNsP12cvOxyjChg6LoFKV0ODMy6fVVKHJHNc\nPxtZe9SSoFxckK5czr4ExVE3EGUUx4jiBCHdku2m4m4eizZdGif5+aMWNU6EEEIIIUODhhMhhBBC\nSCA0nAghhBBCAhlLjZMI1VCs921zzZzGNQgJZBXtjhmn6akTT4tGzeHLMrsTUWTapnLa15pvHUKM\nqp6eqVf1KoXBsnVUq9WwYBTaqtXQpKyW7qUOTVjpNivUMczvThlW2g3qmcgwCbmm5fNU6pXkc0tq\nj4pCvjripWUcx/YX2PFFspP6NsX+cnGeACBD6i8koMeJEEIIISQQGk6EEEIIIYHQcCKEEEIICWQs\nNU5JT1TRXcZC4+SQEsGODCNtQrluz2vKap85Y+aNOzw3b9bYMOdXnzl1xu7VuvPM9Ui0K+JM1BFj\no458WlV0GquRx2s19EpVNCajiHM1jF89Vca3Lk1Tr55RanxGkYtuGJ83F+MZ0ob1nfdLTvxQrzV+\nuIIuCeR9IBZbfDGaACAT+t5YXEF5ni0tXfGW4sS+eGRu12bTfAYnYn8c+z/r6cUT3jJW30ofQQgh\nhBByjkLDiRBCCCEkEBpOhBBCCCGBjKXGSSLjOMUBc7QWMreOY+49k/qiyJwvfezJ48b6+k07jPWn\nF01NFAA0Nz3PWG8vmsnrOh1z3cq1U0UjUEGMUPaQWnRUYxKrp6qmaaWanHHI3TdKTdNqM4w2q1yO\njNM0AqI6vm394tshfXtX5doo//zMxDEyXpL5pCyQOij5fO3tz/N86c/XzywZHE8qROPki0kVAj1O\nhBBCCCGB0HAihBBCCAmEhhMhhBBCSCA0nAghhBBCAlkT4vAwpA1oh/DqJ3UIwjpCDJ6sX2+snzlu\nisM3bnqOsX7coeybaLaM9ShdNNYXF04b6820fMLBKqwVMfgwKNuPgPyVXkYlBK+j3WGIweu4FGS/\nViWxcpVjRiCmd4l1CamLXFyQmeOCk5ukKDvvBsjMo7j4Pzeffa6gmFlq1tHJzGPiVDznHc9T2Y+Q\nIJmSFRlOSqn3AvgogFu11u/q274XwBUANgO4D8A1WusDK2mLEEIIIWTUVJ6qU0r9BIArAXxXbL8B\nwHXdfa8EcBLAvUqpllUJIYQQQsgaopLhpJTaCODPUHiVfiB27wGwT2v9Za31PwF4C4BtAF6/ko4S\nQgghhIyaqlN1dwL4ktb675VSH+htVErtBrAFwFd727TWJ5RS9wO4FMA9VRrLRQDM3KE0yCADcg1O\n8tt2iBUWUpHIMDaHZ/I8U9M09fTTxnpr21arzjNtU9Mk51OtJIW5OTPsmuf1cTZNhcidPJAqmqYh\nxKb0tzmkBKk+sixD1s02nWW5dZ7kGIcEWZNlfP1wJ7teGaMKbplHy9dPXrEfdSTI9g1pSAuy775j\nQsariqaubLhG67NLDVnJPlRmGNd1QFJaXxeM6zOgvkp6uArHlMUK9OxABriMxcVgfdccHc9i+UwW\ndXTXI0SIoshKCozM1ifJ+2wuLuxctOHSL8nPlnbsvvsobTgppd4I4GUAXuHYvQXFEB4V24929xFC\nCCGErFlKGU5KqR0AbgXws1rrtq98ZXbvBjZvBtQlxfoll4gCLrmU/CjSqjbNyihehKTZNI9Z9/wL\njPXznzY/8o7GJmM9fcEuq850/UZjPRZvDkykZ4z1pH3SWD/ZWmfV6eNsv1q27NphLMvV4Z/V9f9a\nWnn0C3lWh/ELLcTbkec5tuzaDgBLS2O/3BD0K69cP8Yl/UyVOizvGoCtO4tx3LpzezXvUZVUTEOg\n7BgOJ/1MxxjPEGwnirwgq1xwVc5J/QOSlazT5aHbtnObsRwGq/FmaBr537ks7XFyVjLY47R1x1Zj\nmUn3kWO2pSFSqjRbpi3QbJp2QCy9WI5+ddLlN9sPHZyxyzuIyrgwlVK/COA/AkixfHUnKO57KYBL\nABwA8DKt9YN9x30NwANa6+uDGkrTHAlfqCWEEELI6nDtv3kb7vyLP/JahWWn6v4OwI+JbZ8F8H0A\nH9NaP6qUmgdwGYAHAUAptQnAq1DoosJQFy97nD73eeSXXw7s37+0O3V4nHKxTVrIeWx6nBZiOyHv\nU03TOj15vulx+s4zprdoumFqnhbOd3geJkyPU5SbXqt1wuMULZoep9MTk1adVdmyawd+Y+978Ic3\n/R7mpw+b/fQebY5NNWdHgNdqjXhZeh6nq/a+C3ff9AnMTR8ZfIDrF5rcVLIfdYzFanmYLBzjsXXn\ndlx90/+Jf7f3Vswfmq3QkdX3OFX57KuT4LjwOL39xutx14c/ibkZz/WJZ7vHqVzf3R6n7XjnTXtw\n+97bMBswnlVYHY9T01vG51GKxXhW8UBdtGs7rn3v23Hnx+7C3KE55AGizUZittNsmJ+lITXDSchM\nyfLsk0wSfNZ+BJXqorU+CeB7/duUUicBPKG1/n53060AblRKHQAwBWAfgMMAvhjc0MGD5vr+/cAD\nD/RtmHAcJLeJExmbBksWmwYLACy2zEE+s9Wc0nr8KdP4Otx6vll+i22MZZOmcRXn5hThZOeUuX/h\nhLF+cnKDVedKmZ8+jEP6UWOb/2ZOw8k4pu8hMj99BNMPPTqgtPvGYomIS05PjYvhVMkQiM9+0NzM\nEcwcmCpdZVTBcFrpEI6z4dRjbuYIph8+OKBsgS87fVmBtauOIIZgPZQ2nAZ0YXbmCKZCxrNUi91j\nVsVw8kcGqsNwsrZFpjna6Aq35w7NYfrAdNCLUE1hCLXEVJ1cDzGE8si2BXzU8RPNGEGt9S0A7gBw\nN4D7AUwCeJ3W2hYVEUIIIYSsIVacckVr/b86tt0M4OaV1k0IIYQQMk6s0Vx1tqNMOvnsPFbmhtSh\ntpfbOiIGRC7mTyHcgrnDLdgR8ymRcEc2hcuzsUr53MpOF9QxQ7ZWc9W5yGDGdVmNNutglHGa+hnG\ncKxGDJxRTc2V/WxjcrmRZym5eFZGjivUmtoVX560uz/Nc3TyzPkWnaQjnuOxiPWUiNx0IVOIjWZ5\nM2g83t8lhBBCCFkD0HAihBBCCAmEhhMhhBBCSCA0nAghhBBCAlmT4nCXQDOXMUeEAEwmEJTC72Kb\necwZkfS3HZvi70iIxbPYIQ4XYeRl2Hkrnk1AwK6y9HpQRsxcB88WQXT5NMurJIYe1xcJRiCGBsZn\njFcqBh9Fsmxy7hByP4vzwXGapPDblTg4Es8+WUeW5kvLtJMjFz2LHV8EWUenI44Rga5d4nAZ26kp\nX/oKgB4nQgghhJBAaDgRQgghhARCw4kQQgghJJA1oXHKIxgCBtccrZwOldqE1Fr3a5xOts0sMSnM\nhLtZw6wjc+TfasuOySJCaxXlYUkGB1FFkyOxtB0BwovV0NysRht1aJrqwKeVcWkAht3msMix+ho8\nZz9G8PmH8XnzaPmz9P+/kn5U6ufYiNWGUOeaxT/AMref76nkymOYi3akbqqXmy7Lsu7/QkfluBOn\nngCXaWr2NHV0PBLthCb27YceJ0IIIYSQQGg4EUIIIYQEQsOJEEIIISSQNaFxkrjm6+VsqIzbJNdT\nR3wHqXs63TZjQnQa5hxtnpjD59JNta3YFOZ6Kg5pOvolqUPDJBmXmEu+zzYMS38Y4xnUroz1NIJz\nUEsC2ipJf1febC2sOMbXGv7s5xa1pCnvWwbUV+XaGpP7sOxIKj5ujMFxnlxI3VRPF5XnObIss/VK\njsFI4NZJLR0jNFAu7ZUkdsR09B5T+ghCCCGEkHMUGk6EEEIIIYHQcCKEEEIICWRNaJwyEcfJhdQq\nyHWpRZDaIsAR60nokWQuurhhrrv0Ep2OiDuRm7qpFOb+dmauZ3H5PDqrQR2aqFFpi+pAxsk5l1ir\nn3et9puQcUPeu2WMJjcr/wJaulChcZKaJpfGKUSP5YMeJ0IIIYSQQGg4EUIIIYQEQsOJEEIIISSQ\nNaFxkrjjOA22Ae3ca67yMvaTuTeSeeUSc93ZLzEH2xEapk5kriciDgVNW0LGh1Hl8vMh+8XbBhl3\n+uM45XkepD2yNEvimJC4TXXA7xchhBBCSCA0nAghhBBCAqHhRAghhBASCA0nQgghhJBAxlIcvqSn\nzgrLLsoiQ/242LAFYE+1zG1Prpsw1h+feL6x/sS69VYdT0xsMNaPCcXlaXFMDrONTYttq871VuDN\nlrkhW2esdpLzjPU4PWXVWZYl0XpfxMYoM4N3+mR5eSTKB2jwbEG+uSFBeaLIHmNvPzz7ff1wif6j\nPF9K2BznGRIZLFW+WOBKWCl1jiUDxOV56i8kkAJM2QdEAWfFGtDBL1W46xAvVgCIuvVEiCupsDPf\nRencbbbjDcrq+exAQKDNvMJ4+bCupQRRVtxroqyFKJtwHCTr8I1f+evNIuTG4f3GVgmdW26QY0cX\n4r6la3/pawmwrgWrl7G/31HId9Yov2B3QwaOzAbvt677gJet5L2nETWWlk1noGfXCMrxkAEwxctX\nqf1Zo7ZIDNwJ+G4I6HEihBBCCAmEhhMhhBBCSCA0nAghhBBCAhlLjVPeS6bbXbaTBHmy3NXFhm3v\ntUUwykURrLIjDpEJfQEgE3PraTR4zlrO2boCeMktidgiEwnnzrniccBK6ziSXpB6kXqcKnKbKhqd\n1QlTRwipA/lsqyPOZN69C+TIzxK40t6Wi+dlJDRzMuC0XAeAVASZlushlDKclFIfBPBBsXm/1vpH\n+srsBXAFgM0A7gNwjdb6QOmeEUIIIYSMGVXcBv8E4EIAW7p//1Nvh1LqBgDXAbgSwCsBnARwr1Kq\n5aiHEEIIIWRNUWWqrqO1fuws+/YA2Ke1/jIAKKXeAuAogNcDuKdaFwkhhBBCxoMqhtPFSqkjAM4A\n+AcA79NaH1JK7Ubhgfpqr6DW+oRS6n4Al6KE4fRMNwZTPNHCeQBOt1ro9MVlOtWwHVgnE/OjnGqY\n66eFBup0ZDvbFqSmSWg3rPgsIsZG7PDfWZqmzJyTteOBiDnZyBHLohYd1Ag0S5WCP8n9Ffo5lMSs\nOURkF3t36U6UjOMUVYirU2EsymqYvHGMAK/IKagOQgKxY555ksI76zCT0nrbdGyzdLDy/l7p3l7/\nvdyfcFfud5UfnIDXl+TXNcY5ZB2DNU0u/ZJsp9PpOPo+mLIj/P8B+HUArwVwNYDdAP5fpdQGFEZT\njsLD1M/R7j5CCCGEkDVNKY+T1vrevtV/Ukp9E8A0gF8BsL+uTkUvfCGizZsR//APAwCS7rJHq2FH\nGZ2MzY+ysWl6pbKmGaE7apjrRb1mBNF1wmJeFHVkLbN85oh+msposrl8C8A6xOynI8pr2Z/jveJb\ndu0wliYeG9r6BeLvQ7UXLwbXG1WIGrzSF0Bcw53nObbs2g4AS8uBbXp/wQUVEf0q/0vJ10jkiKPu\nv9zEL8WgjtjX29ad25aWq/fWXYVozwOOP9smg2FEDhckyI3xDMHvRakSsdtqpYY6yvdD3nd9J8nV\ny207txtLG/E9cDbhO9niexFwbdjeIY83zZF9wTr3ni99bu13lRfZAUQb27vXZW9pe5zs85xDepDM\nOhuJef9qNO37WSLKbDpv49L/R6YOW+VdRCEux0F0jae/BfAZAI8AeJnW+sG+/V8D8IDW+vrQOvM0\nzaOkSkIOQgghhJDyvOvN78AnPneH11xdURwnpdRGAC8G8Mda64NKqXkAlwF4sLt/E4BXAbizTL2H\nf+IViJ/zHDQvvhgX/P4f4MhVV2Px4YeX9i848vecSkxvzzNCB3UiMdefSRw6KeFReloEe0onTA9T\nLsr34k7105E5uaQVnco5bvELOCmfm02Sd38NbNm1A1fsezc+84GPY37aZ1nLXz7lfxbnQxAXRRV+\nsQ7T43TV3nfh7ps+gfnpI4PbHJHHycqZV8Xj5G2livfR7XG65qY9+PTe2zA3M+utU342d/4wH89O\nj1McFR6na2+8Dnd++FNB4+kLzuPyAJSnBo+TS/fpazUr167r3rVt53a844PX444PfRKzM0ccR4k6\nnN+11fA4DSaD/74RebxFtjfJVcvgOnbs2o49N12H2/Z+Ckec16fL4yS3meux0C6HeJw2bljOQRuq\ndyobx+l3AXwJxfTcdgAfAtAG8IVukVsB3KiUOgBgCsA+AIcBfLFMO53paWN98eGHsfDgkhPLEnoD\nwElhCJ0QRs2TYhruKcdU3YnWpLH+A2E4ddaZ+/OWSBSc2MMpDScrIJfXcHJM1ZUkFxfw/PRhHNKP\neo4SF30Vw8khwF8pUYWb90qFxmcznHrMTx/BzEPmePqCpboofQMMSHhci+Hk7Zb/PIcYTj3mZmYx\n/fBBb53jYTiNSZJfQdL3IkYxnlPeY3yzD1WSSjtqWXkVVQwneZ/1lR9wfc7OHMHUQyHXpyMgsu87\nLsXhQUl+y11AKfz3jdKGk/MiHlxHr99HZmZx8KEpx/EhU3XCcBJvaDVb9jO5IV4ce07fVF0oZT1O\nOwB8HsDzATwG4OsAflJr/QQAaK1vUUqtB3A3igCY/xXA67TWi6V7RgghhBAyZpQVh78poMzNAG6u\n2B9CCCGEkLGFCccIIYQQQgIZyyS/cxOFXmlDq4ntAJ5oJnhmYrmrC47X/k8JfdHTDVOr8bTYf9KR\nKPiM2NQRGh07MbBIOOjQCERZW6yLAnVkS/TSazQXy5Bjush59KBAbcMIsjkCW98dDQ/L+pjIHg/r\nWqk/AOZQkusG1GlpeAIup5C+9spk0XB0P+TcpawOyBWst1dHFEWBmsVyTRYHyXvm6rxdbn0e8fGl\ndsgKR+D4sL5HW9S92USIgsfUd1+V/XTVOYoAmIQQQggh5yw0nAghhBBCAqHhRAghhBASyFhqnA51\n9UnP7cZreqwR43ifZqntyKZ7WgS1OmMl/Y3Ffnvu+IyImdERc7SW7kLEFJJ6JsAVyl7EssDgOdm0\nUtyTkMl1X1wTXwqWkLgoso46AujR1u9Rx5URlDnBwzC0VquXcoWcC0jtiw9XLK5eFXHcjXXs03k6\n6shKBuIcSTJ22M8hWyu0co2TD+c589xr5DEy2KWrjCsRsLdvpY8ghBBCCDlHoeFECCGEEBIIDSdC\nCCGEkEDGUuN0uDsHudiNo3QsinG0b14yc+SEa4u5zEURxykV+WkWHXGcFkXcpo6Y6M5FbI8oF/Ef\nMocdmklNk0nsSVJYDTG5vBRuKF9eyjgl1qS+lW1NrFeJ4ySp8lmHEbyoapN9cZy8/ao/jlOlHHxV\nNEwlywf1y3NOquimEgqjyFkoH8fJWcnSMooixz1U6lVdFfvuqz5c38Yq9+aVIcezylcv7x6VI3fm\nSXRppOSY+rRYIXGcrPyxAdDjRAghhBASCA0nQgghhJBAaDgRQgghhAQylhqno3GhT2p0dU3HkwYe\nS/ry0zliM6SJmb8uF5qmTByTxXYdcpuc+8w9sYsszVNxkChj1iHjTESOfq2cAPvYkZtJFBDrVeI4\nSR4KCuIAABDjSURBVKrEdVqd3E3nKlXOSCWtlWw3MnPVkZWRwcxMGXJevbG11vB5aXjvbwKH1tSb\nq05qYIP8ErJfdQzyyuPl+bRCdaRY7T1fsyxDlmWOfHiOfq28WQuXvsoHPU6EEEIIIYHQcCKEEEII\nCYSGEyGEEEJIIGOpccqed36x3LQZAJCedx7S5z53aX/umOnsQMaVMD9aKmM3OKY1s9zMWWNNfcrc\ndNacto2cCo9FPrwUos2OuZ7G/lndSOh+rPn3JW1W/1J+lsGxQNIsJJ+PjHtVQ246GRslL59XaKXa\nDHc8kQimimT1gwiVjk0TckyFOrMqggdxTI5lrUGeu+O6+FnDIpya6elG5P8r4lk0vL7vgSucXu9W\nHEdFvrrcd2PJ7TFvtVrGujwvtq5W9Mvh67D1R/nAdVf+Npmvrd0x867KNhqNlli3TQk5PvKzNZvN\npaUcl6JNR2wn+byscJ+Qn6VK7ER6nAghhBBCAqHhRAghhBASCA0nQgghhJBAaDgRQgghhAQyluLw\nE10V4ubu8nQe4Zl+oVnkD4KYS5NQBqJ0iq7PJqru1SGEe1bsMr96UsoFY9GmHfwvIOCj6Kcll+t9\n9l7/HaJFG1HGGguXzV02iGYAVrvlqxgFVYTbw6BsP1xn1Xe1ePNFh7SbF3+9/wlZTaokrZXiZa9Y\nHIDv22QJu61GHcc778XlsANe+vb7k+nGIpizDHDZE2XHUWztKwhJaOwR+Qck+a1yp6bHiRBCCCEk\nEBpOhBBCCCGB0HAihBBCCAlkLDVOp7sTu2ewvDzdN9mbnDUg4TKxmG+OA2zESM6pWlEyRcBLT+Ax\n+wj7mNQzmx4W30vONwdowKxAkoMDYNpBNUM6Vr9d7gqK5j9opY26+rGsA4giIIqlNsHfT3v+ffD1\nJZH6OCe+bgQEWE08/bCD9PnJB2gPoihC8myKtkhGjvx+Wl8MS7Nj30Nlkl/7fi+DCvuDVQLlA/ra\nlXrTM4s++PtlB24e/DxwBdWMY9O8sINoNpaWzWbTGs8s61h1ZpY+d7DWyqWdsj5LkOZX1FH6CEII\nIYSQcxQaToQQQgghgdBwIoQQQggJZCw1Tsv2XLGMkJhzzs64FWJSVgREisQ8b4C0A6nQAVnz0zJ+\njaOO3NOQN/SHHdgpQFsl5s275XsJEnOkduwna0zddSwTYnP75o4r2O11JA6uhQiDkiYHSXR82iGr\n+Mrjj1h1hkjG5PUlrumQuEvyMvb1nQqnlZFHfeHbooD7zGrh1AuW7Zw/hpDdxuCL1I7j5L/PeBMF\nO9r0x0OSMflW58TZ2qBy2iGXlkgm/pU6qKSRLC0bjYb1HEtTxzmTSZDFMT4tlmtbVGGMSxtOSqlt\nAH4HwOsArAfwMIC3aq2/3VdmL4ArAGwGcB+Aa7TWB0r3jhBCCCFkjCj1k18p1TOEFgC8FsBLALwb\nwPG+MjcAuA7AlQBeCeAkgHuVUq2a+kwIIYQQMhLKepzeC2BGa31F37ZpUWYPgH1a6y8DgFLqLQCO\nAng9gHuqdpQQQgghZNSUNZx+HsBXlFL3APgZAEcA3KW1/gwAKKV2A9gC4Ku9A7TWJ5RS9wO4FIGG\nUzNuFp2Liu4lUbL0P+DWP0idhYzJJKebQ7QdiSeIktRtuKRXMn+R1BpYM+kyPoZjrt0f20nE1MjN\n4/Lcr4uKRB1eHRUA6cD0D3H5GCZV5qOHIRMoxjDr/p9ZcbF8+qReHYOwtWxif4Dcy9IWWTo9KdRz\ndErq9ER8s5BzEqKD6s9VV0U3RcYPS08SIi711FF2f1Fm8JclSGfVe2hEeVe/NHjCJqhKj3bIzhln\nx0uy8tv51jN7LHzaK592yJ2rbrAOqlKuOnEeM3kvChh0OR6rkavuRQCuAaABvAbApwHcrpT61e7+\nLSieGUfFcUe7+wghhBBC1ixlPU4xgG9qrT/QXf+uUupHAVwN4E/r6tQLtm/D5MaN2PrCnQCwtOzh\n8hZJC9CKAi5fDAr4ReuKBG7slz/EXR4nDPY4WW+7SEs/9UcBl0hvUc9jsGXXRUtLn2Eu6wj7dV8+\nw3hZ4govgq60H2c7fsuuHcbSOKZcEm8nvl9PIW9K2VG9fQc5LmLPISFZ4e0xtNvZunObsfQhr8mQ\n77SNWUn5dzZdr4l5DhGu6WF4zjKk2NYdx22B4+mn/ADbHqfyrTp8tQPbcNEo6XFyfdKtF20zllbP\nxEGu8xqJ6z7zvCVme67tAfQdI9twvVLuuy/Eot04EW/MdWeJ+mk0mqKM+Ux5wdbzAQBbdlzobDN1\nRQ4X27LcXJf9TBqOyOHi7b64b8QOPTrj7Isk8hkH/SilpgD8jdb6yr5tVwN4v9b6ou5U3SMAXqa1\nfrCvzNcAPKC1vj6knSxNc/nhCCGEEEKGxTX/+gp8+j9+xmuJl/35fh8AJbYpdAXiWuuDSql5AJcB\neBAAlFKbALwKwJ2hjbzvl9+05HG66sM34+4bb8bc1LIlSI/TYAZ5nK788Pvx+zd+BEdnDpWqgx4n\nmy27duCqve/B3Tf9HuanD5vH0ONklgloZ+vObbjmpj349N7bMDcz662THqez0/M4vfMD78Tt+27H\nbMB4+qHH6ar37sHdH7sNc4dm7Z7R42QQ4nG64t1vxWc+/u8xf1iqe0bjcQql7FPokwDuU0q9D4XQ\n+1Uo4jX9Zl+ZWwHcqJQ6AGAKwD4AhwF8MbSR44fncBxAo3tDfuzgIczqh5f2O8XhIliWJQ63BK3+\nW2Qmkw6KhlP5vUnsnskyqbzwxUUs22gt+k+R98bRbaMnYJ6bmsKhhx4tVUdaIYKedYgzcGk5kqiC\n4bTCB5Pr+DzPlwKKzk1PY1o/Io6pXwRr9ytEFFuuTsvoRsiDXbwUENKk+00KAMDc9BFMPXwwoBKT\n8oEUgZUbTo4H2ZgYTj1mZ2Yx9fBULbWWxUr+WiEopF2+fALtJGuXatPVQq/M/OE5HHpkytovf0C4\nErjbL2eI744n2KwL2W6nYxoTaWq+uJK5fsYIcXYSDw5eKUXqbsPJjEAk60izol/zR47i0EHzhyfg\nSkQPpOI8pmnbKtNP7Hgmy340KrywUOpJprX+RwC/BOBNAP47gPcD2KO1/kJfmVsA3AHgbgD3A5gE\n8Dqt9WLp3hFCCCGEjBGlf75rrf8awF97ytwM4OZqXSKEEEIIGU+Y5JcQQgghJJCxTPKbd7rJaHtz\ns50UUXt5vtM1xS01S7FMBCn3B4nDhU7KkieZGzoOMbnckscigJdV3jPnHUBYEDA5fzxYhC7HIsTm\ntnsu2qygecqj8kEzKwyhebyzznxgAEw5WmHnpOQxmf/FgdwjpLVeVrDOs78fmfxChny3HIV613qe\n51aQzSAcegYyWqzAhkPQOAUFwCwpdHQmh+1bRlHkvTeHBeb0BJpM/PdIqWHyJYHvaYsGHVP2nLjG\nwhKle7Db8AfVlMnpfWJ7wB6vxBFU1Ac9ToQQQgghgdBwIoQQQggJhIYTIYQQQkggpSKHE0IIIYSc\ny9DjRAghhBASCA0nQgghhJBAaDgRQgghhARCw4kQQgghJBAaToQQQgghgdBwIoQQQggJhIYTIYQQ\nQkggNJwIIYQQQgKh4UQIIYQQEggNJ0IIIYSQQGg4EUIIIYQE0hh1B86GUupaAO8BsAXAdwG8Q2v9\n30bbq/FHKfU+AL8E4BIApwF8A8ANWuuHRLm9AK4AsBnAfQCu0VofWOXurjmUUu8F8FEAt2qt39W3\nneMZiFJqG4DfAfA6AOsBPAzgrVrrb/eV4XgGoJSKAXwIwJtR3CtnAXxWa/1hUY7j6UAp9WoAvwXg\n5QC2Ani91vqvRJmBY6eUmgDwCQBvADAB4F4Ab9daH1uVDzFmDBpTpVQDwEdQfPdfBOApAH8H4L1a\n67m+OsZ6TMfS46SUegOAjwP4IIAfR2E43auUOn+kHVsbvBrAHQBeBeBnATQB/I1SarJXQCl1A4Dr\nAFwJ4JUATqIY39bqd3ftoJT6CRRj9l2xneMZiFKq9/BZAPBaAC8B8G4Ax/vKcDzDeS+AqwC8HcWP\npd8G8NtKqet6BTieA9kA4Dsoxs/KeB84drcC+N8B/DKAnwawDcBfDrfbY82gMV0P4GUojP0fR/Ej\nXwH4oig31mM6rh6n6wHcrbX+EwBQSl2NYhDfBuCWUXZs3NFa/1z/ulLq1wEcQ2H9f727eQ+AfVrr\nL3fLvAXAUQCvB3DPqnV2DaGU2gjgz1D88vyA2M3xDOe9AGa01lf0bZsWZTie4VwK4Ita669012eU\nUpejeMj34Hiehe64fQUAlFKRo8jAsVNKbULxXHqj1vq/dMu8FcD3lVKv1Fp/cxU+xlgxaEy11idQ\n/GBaomvk36+U2qG1PrwWxnTsPE5KqSaKh/xXe9u01jkKd96lo+rXGmYzCqv/SQBQSu1G4dLvH98T\nAO4Hx3cQdwL4ktb67/s3cjxL8/MA/lEpdY9S6qhS6ttKqSUjiuNZmm8AuEwpdTEAKKVeCuCnAPx1\nd53jWZHAsXsFCgdEfxkNYAYc31B6z6gfdNdfjjEf07EznACcDyBBYdX3cxTFRUwC6Vr7twL4utb6\ne93NW1BcpBzfQJRSb0ThXn6fYzfHsxwvAnANAA3gNQA+DeB2pdSvdvdzPMvxMQB/DmC/UmoRwLdQ\n6O++0N3P8axOyNhdCGCxa1CdrQw5C10t08cAfF5r/Ux38xaM+ZiO61QdqYe7APwIil+gpAJKqR0o\njM+f1Vq3R92fZwExgG9qrXvTnd9VSv0ogKsB/OnourVmeQOAywG8EcD3UBj4tymlZrXWHE8ytnSF\n4n+Bwjh9+4i7U4px9Dg9DiBFYcn3cyGA+dXvztpEKfUpAD8H4H/uf1sBxRhG4PiG8nIALwDwbaVU\nWynVBvAzAPZ0f+EfBcezDHMAvi+2fR/Azu7/vD7LcQuAj2mt/0Jr/c9a688B+CSWvaMcz+qEjN08\ngFZXl3O2MkTQZzRdBOA1fd4mYA2M6dgZTt1f9d8CcFlvW3fK6TIU8/nEQ9do+kUA/4vWeqZ/n9b6\nIIqLr398N6F4C4/ja/N3AH4MxS/5l3b//hGFUPylWutHwfEsw30o3qLpR6ErEOf1WZr1KH5o9pOh\ne2/neFYncOy+BaAjyigUPwT+YdU6u4boM5peBOAyrfVxUWTsx3Rcp+o+AeCzSqlvAfgmirfs1gP4\n7Cg7tRZQSt0F4E0AfgHASaVU79fSU1rrM93/bwVwo1LqAIApAPsAHIb9Sug5j9b6JIopkCWUUicB\nPKG17nlOOJ7hfBLAfd14Y/egeAhdAeA3+8pwPMP5EoqxOgzgnwH8SxT3y8/0leF4ngWl1AYAL0bh\nWQKAF3UF9k9qrQ/BM3Za6xNKqT8E8Aml1HEATwO4HcB94/D21ygYNKYoPM5/ieKH6L8C0Ox7Rj2p\ntW6vhTEdO48TAGit70ER/HIvgAcA/A8AXqu1fmykHVsbXA1gE4CvoQiG1/v7lV4BrfUtKGI93Y3i\nDZFJAK/TWi+udmfXKEZsEo5nOFrrf0QRu+VNAP47gPcD2NMnZuZ4luM6AP8Pirc+v4di6u7TAG7q\nFeB4DuQVKJ4x30Lxvf44gG+jiDMUOnbXA/gyivPwNRT3219ene6PJYPGdDuKN2t3oIj1NIvCmJqF\n+cbcWI9plOdWzC9CCCGEEOJgLD1OhBBCCCHjCA0nQgghhJBAaDgRQgghhARCw4kQQgghJBAaToQQ\nQgghgdBwIoQQQggJhIYTIYQQQkggNJwIIYQQQgKh4UQIIYQQEggNJ0IIIYSQQGg4EUIIIYQE8v8D\nIStzktWLgmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe00ba27b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ims = os.listdir(TRAIN_CROP_DIR+'NoF')\n",
    "im = Image.open('../data/train_crop/NoF/'+ims[2])\n",
    "imshow(np.asarray(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = Image.open('../data/train_crop/ALB/SHARK_img_06082_2_ALB.jpg')\n",
    "imshow(np.asarray(im))\n",
    "\n",
    "im_sizes = []\n",
    "for c in crop_classes:\n",
    "    TRAIN_CROP_DIR_c = TRAIN_CROP_DIR + '{}/'.format(c)\n",
    "    files = glob.glob(TRAIN_CROP_DIR_c+'*')\n",
    "    for file in files:\n",
    "        im = Image.open(file)\n",
    "        #size = (width, height)\n",
    "        size = im.size\n",
    "        im_sizes.append(size)\n",
    "im_sizes = np.asarray(im_sizes)\n",
    "\n",
    "len(im_sizes)\n",
    "\n",
    "np.mean(im_sizes[:,1]/im_sizes[:,0])\n",
    "\n",
    "plt.hist(im_sizes[:,1]/im_sizes[:,0], bins=10)\n",
    "\n",
    "plt.scatter(im_sizes[:,0],im_sizes[:,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist data_train_BBCrop_270_480.pickle. Loading data from file.\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "import pickle\n",
    "\n",
    "def get_images(fish):\n",
    "    \"\"\"Load files from train folder\"\"\"\n",
    "    fish_dir = TRAIN_CROP_DIR+'{}'.format(fish)\n",
    "    images = [fish+'/'+im for im in os.listdir(fish_dir)]\n",
    "    return images\n",
    "\n",
    "def read_image(src):\n",
    "    \"\"\"Read and resize individual images\"\"\"\n",
    "    im = Image.open(src)\n",
    "    im = im.resize((COLS, ROWS), Image.BILINEAR)\n",
    "    im = np.asarray(im)\n",
    "    return im\n",
    "\n",
    "if os.path.exists('../data/data_train_BBCrop_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_train_BBCrop_{}_{}.pickle. Loading data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_train_BBCrop_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    X_train = data_train['X_train']\n",
    "    y_train = data_train['y_train']\n",
    "else:\n",
    "    print ('Loading data from original images. Generating data_train_BBCrop_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "    \n",
    "    files = []\n",
    "    y_train = []\n",
    "\n",
    "    for fish in FISH_CLASSES:\n",
    "        fish_files = get_images(fish)\n",
    "        files.extend(fish_files)\n",
    "\n",
    "        y_fish = np.tile(fish, len(fish_files))\n",
    "        y_train.extend(y_fish)\n",
    "        #print(\"{0} photos of {1}\".format(len(fish_files), fish))\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    X_train = np.ndarray((len(files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(files): \n",
    "        X_train[i] = read_image(TRAIN_CROP_DIR+im)\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "    #X_train = X_train / 255.\n",
    "    #print(X_train.shape)\n",
    "\n",
    "    # One Hot Encoding Labels\n",
    "    y_train = le.transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    \n",
    "    #save data to file\n",
    "    data_train = {'X_train': X_train,'y_train': y_train }\n",
    "\n",
    "    with open('../data/data_train_BBCrop_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_train, f)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=None, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=[0.9,1.1],\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "#train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BatchSize, shuffle=True, seed=None)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator = valid_datagen.flow(X_valid, y_valid, batch_size=BatchSize, shuffle=True, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')        \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='./checkpoints/weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        \n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.9705 - acc: 0.3609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 2.19270, saving model to ./checkpoints/weights.000-2.1927.hdf5\n",
      "3904/3868 [==============================] - 177s - loss: 1.9661 - acc: 0.3619 - val_loss: 2.1927 - val_acc: 0.4805\n",
      "Epoch 2/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.5559 - acc: 0.4984Epoch 00001: val_loss improved from 2.19270 to 1.90339, saving model to ./checkpoints/weights.001-1.9034.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 1.5496 - acc: 0.5015 - val_loss: 1.9034 - val_acc: 0.5322\n",
      "Epoch 3/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 1.3942 - acc: 0.5461Epoch 00002: val_loss improved from 1.90339 to 1.83117, saving model to ./checkpoints/weights.002-1.8312.hdf5\n",
      "3924/3868 [==============================] - 168s - loss: 1.3981 - acc: 0.5456 - val_loss: 1.8312 - val_acc: 0.5273\n",
      "Epoch 4/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.2804 - acc: 0.5927Epoch 00003: val_loss improved from 1.83117 to 1.80418, saving model to ./checkpoints/weights.003-1.8042.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 1.2819 - acc: 0.5935 - val_loss: 1.8042 - val_acc: 0.5186\n",
      "Epoch 5/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.2488 - acc: 0.5982Epoch 00004: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 1.2502 - acc: 0.5978 - val_loss: 1.8170 - val_acc: 0.5088\n",
      "Epoch 6/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 1.1507 - acc: 0.6192Epoch 00005: val_loss improved from 1.80418 to 1.73375, saving model to ./checkpoints/weights.005-1.7337.hdf5\n",
      "3924/3868 [==============================] - 166s - loss: 1.1451 - acc: 0.6205 - val_loss: 1.7337 - val_acc: 0.5361\n",
      "Epoch 7/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.1074 - acc: 0.6359Epoch 00006: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 1.1076 - acc: 0.6360 - val_loss: 1.7609 - val_acc: 0.5273\n",
      "Epoch 8/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.1134 - acc: 0.6260Epoch 00007: val_loss improved from 1.73375 to 1.72027, saving model to ./checkpoints/weights.007-1.7203.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 1.1062 - acc: 0.6294 - val_loss: 1.7203 - val_acc: 0.5625\n",
      "Epoch 9/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 1.0785 - acc: 0.6376Epoch 00008: val_loss improved from 1.72027 to 1.67434, saving model to ./checkpoints/weights.008-1.6743.hdf5\n",
      "3924/3868 [==============================] - 165s - loss: 1.0757 - acc: 0.6381 - val_loss: 1.6743 - val_acc: 0.5742\n",
      "Epoch 10/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.0294 - acc: 0.6505Epoch 00009: val_loss improved from 1.67434 to 1.64761, saving model to ./checkpoints/weights.009-1.6476.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 1.0269 - acc: 0.6516 - val_loss: 1.6476 - val_acc: 0.5684\n",
      "Epoch 11/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 1.0385 - acc: 0.6477Epoch 00010: val_loss improved from 1.64761 to 1.59848, saving model to ./checkpoints/weights.010-1.5985.hdf5\n",
      "3904/3868 [==============================] - 165s - loss: 1.0396 - acc: 0.6483 - val_loss: 1.5985 - val_acc: 0.5674\n",
      "Epoch 12/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.9993 - acc: 0.6655Epoch 00011: val_loss improved from 1.59848 to 1.48698, saving model to ./checkpoints/weights.011-1.4870.hdf5\n",
      "3924/3868 [==============================] - 165s - loss: 0.9992 - acc: 0.6659 - val_loss: 1.4870 - val_acc: 0.5361\n",
      "Epoch 13/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.9626 - acc: 0.6852Epoch 00012: val_loss improved from 1.48698 to 1.34949, saving model to ./checkpoints/weights.012-1.3495.hdf5\n",
      "3904/3868 [==============================] - 165s - loss: 0.9625 - acc: 0.6844 - val_loss: 1.3495 - val_acc: 0.5811\n",
      "Epoch 14/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.9470 - acc: 0.6716Epoch 00013: val_loss improved from 1.34949 to 1.26585, saving model to ./checkpoints/weights.013-1.2659.hdf5\n",
      "3904/3868 [==============================] - 165s - loss: 0.9450 - acc: 0.6719 - val_loss: 1.2659 - val_acc: 0.6191\n",
      "Epoch 15/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.9604 - acc: 0.6705Epoch 00014: val_loss improved from 1.26585 to 1.08836, saving model to ./checkpoints/weights.014-1.0884.hdf5\n",
      "3924/3868 [==============================] - 167s - loss: 0.9599 - acc: 0.6702 - val_loss: 1.0884 - val_acc: 0.6543\n",
      "Epoch 16/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.9059 - acc: 0.6865Epoch 00015: val_loss improved from 1.08836 to 0.99485, saving model to ./checkpoints/weights.015-0.9948.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 0.9104 - acc: 0.6849 - val_loss: 0.9948 - val_acc: 0.6729\n",
      "Epoch 17/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.9197 - acc: 0.6839Epoch 00016: val_loss improved from 0.99485 to 0.86695, saving model to ./checkpoints/weights.016-0.8670.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 0.9172 - acc: 0.6842 - val_loss: 0.8670 - val_acc: 0.6963\n",
      "Epoch 18/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.8863 - acc: 0.6935Epoch 00017: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.8884 - acc: 0.6922 - val_loss: 0.8839 - val_acc: 0.6963\n",
      "Epoch 19/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.8509 - acc: 0.7034Epoch 00018: val_loss improved from 0.86695 to 0.78527, saving model to ./checkpoints/weights.018-0.7853.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 0.8505 - acc: 0.7039 - val_loss: 0.7853 - val_acc: 0.7266\n",
      "Epoch 20/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.8876 - acc: 0.6951Epoch 00019: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.8857 - acc: 0.6954 - val_loss: 0.8274 - val_acc: 0.7188\n",
      "Epoch 21/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.8346 - acc: 0.7145Epoch 00020: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.8364 - acc: 0.7128 - val_loss: 0.7999 - val_acc: 0.7266\n",
      "Epoch 22/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.8515 - acc: 0.7008Epoch 00021: val_loss improved from 0.78527 to 0.77103, saving model to ./checkpoints/weights.021-0.7710.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 0.8498 - acc: 0.7016 - val_loss: 0.7710 - val_acc: 0.7402\n",
      "Epoch 23/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.8409 - acc: 0.7060Epoch 00022: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.8423 - acc: 0.7052 - val_loss: 0.7944 - val_acc: 0.7266\n",
      "Epoch 24/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.8067 - acc: 0.7171Epoch 00023: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.8073 - acc: 0.7171 - val_loss: 0.7824 - val_acc: 0.7363\n",
      "Epoch 25/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.8226 - acc: 0.7206Epoch 00024: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.8275 - acc: 0.7195 - val_loss: 0.7812 - val_acc: 0.7354\n",
      "Epoch 26/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.8119 - acc: 0.7276Epoch 00025: val_loss improved from 0.77103 to 0.73947, saving model to ./checkpoints/weights.025-0.7395.hdf5\n",
      "3904/3868 [==============================] - 167s - loss: 0.8105 - acc: 0.7282 - val_loss: 0.7395 - val_acc: 0.7500\n",
      "Epoch 27/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7953 - acc: 0.7207Epoch 00026: val_loss improved from 0.73947 to 0.70311, saving model to ./checkpoints/weights.026-0.7031.hdf5\n",
      "3924/3868 [==============================] - 166s - loss: 0.8008 - acc: 0.7187 - val_loss: 0.7031 - val_acc: 0.7666\n",
      "Epoch 28/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7741 - acc: 0.7286Epoch 00027: val_loss did not improve\n",
      "3904/3868 [==============================] - 164s - loss: 0.7756 - acc: 0.7277 - val_loss: 0.7623 - val_acc: 0.7363\n",
      "Epoch 29/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7764 - acc: 0.7365Epoch 00028: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.7788 - acc: 0.7351 - val_loss: 0.7383 - val_acc: 0.7480\n",
      "Epoch 30/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7663 - acc: 0.7342Epoch 00029: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.7625 - acc: 0.7360 - val_loss: 0.7240 - val_acc: 0.7656\n",
      "Epoch 31/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7393Epoch 00030: val_loss did not improve\n",
      "3904/3868 [==============================] - 166s - loss: 0.7493 - acc: 0.7392 - val_loss: 0.7312 - val_acc: 0.7549\n",
      "Epoch 32/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7820 - acc: 0.7297Epoch 00031: val_loss improved from 0.70311 to 0.67302, saving model to ./checkpoints/weights.031-0.6730.hdf5\n",
      "3904/3868 [==============================] - 166s - loss: 0.7832 - acc: 0.7285 - val_loss: 0.6730 - val_acc: 0.7803\n",
      "Epoch 33/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7446 - acc: 0.7409Epoch 00032: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.7447 - acc: 0.7406 - val_loss: 0.6791 - val_acc: 0.7646\n",
      "Epoch 34/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7589 - acc: 0.7339Epoch 00033: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.7598 - acc: 0.7341 - val_loss: 0.7013 - val_acc: 0.7764\n",
      "Epoch 35/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7296 - acc: 0.7461Epoch 00034: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.7335 - acc: 0.7459 - val_loss: 0.7264 - val_acc: 0.7676\n",
      "Epoch 36/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7205 - acc: 0.7464Epoch 00035: val_loss improved from 0.67302 to 0.65191, saving model to ./checkpoints/weights.035-0.6519.hdf5\n",
      "3924/3868 [==============================] - 167s - loss: 0.7189 - acc: 0.7467 - val_loss: 0.6519 - val_acc: 0.7881\n",
      "Epoch 37/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7350 - acc: 0.7411Epoch 00036: val_loss did not improve\n",
      "3904/3868 [==============================] - 166s - loss: 0.7328 - acc: 0.7413 - val_loss: 0.6706 - val_acc: 0.7773\n",
      "Epoch 38/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7106 - acc: 0.7513Epoch 00037: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.7085 - acc: 0.7520 - val_loss: 0.6896 - val_acc: 0.7725\n",
      "Epoch 39/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7237 - acc: 0.7484Epoch 00038: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.7213 - acc: 0.7482 - val_loss: 0.6904 - val_acc: 0.7705\n",
      "Epoch 40/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7082 - acc: 0.7529Epoch 00039: val_loss did not improve\n",
      "3904/3868 [==============================] - 166s - loss: 0.7083 - acc: 0.7526 - val_loss: 0.6980 - val_acc: 0.7627\n",
      "Epoch 41/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7521Epoch 00040: val_loss improved from 0.65191 to 0.60545, saving model to ./checkpoints/weights.040-0.6054.hdf5\n",
      "3904/3868 [==============================] - 167s - loss: 0.7116 - acc: 0.7518 - val_loss: 0.6054 - val_acc: 0.7910\n",
      "Epoch 42/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7075 - acc: 0.7575Epoch 00041: val_loss did not improve\n",
      "3924/3868 [==============================] - 166s - loss: 0.7064 - acc: 0.7576 - val_loss: 0.6484 - val_acc: 0.7891\n",
      "Epoch 43/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7045 - acc: 0.7529Epoch 00042: val_loss did not improve\n",
      "3904/3868 [==============================] - 166s - loss: 0.7059 - acc: 0.7523 - val_loss: 0.6697 - val_acc: 0.7793\n",
      "Epoch 44/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.7591Epoch 00043: val_loss did not improve\n",
      "3904/3868 [==============================] - 166s - loss: 0.6976 - acc: 0.7572 - val_loss: 0.6519 - val_acc: 0.7734\n",
      "Epoch 45/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.7459Epoch 00044: val_loss did not improve\n",
      "3924/3868 [==============================] - 164s - loss: 0.6907 - acc: 0.7452 - val_loss: 0.6723 - val_acc: 0.7783\n",
      "Epoch 46/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.7557Epoch 00045: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.7128 - acc: 0.7549 - val_loss: 0.6645 - val_acc: 0.7861\n",
      "Epoch 47/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.6747 - acc: 0.7667Epoch 00046: val_loss did not improve\n",
      "\n",
      "Epoch 00046: reducing learning rate to 9.999999747378752e-06.\n",
      "3904/3868 [==============================] - 166s - loss: 0.6741 - acc: 0.7669 - val_loss: 0.6624 - val_acc: 0.7754\n",
      "Epoch 48/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.6682 - acc: 0.7681Epoch 00047: val_loss did not improve\n",
      "3924/3868 [==============================] - 167s - loss: 0.6680 - acc: 0.7686 - val_loss: 0.6657 - val_acc: 0.7744\n",
      "Epoch 49/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.6695 - acc: 0.7680Epoch 00048: val_loss did not improve\n",
      "3904/3868 [==============================] - 165s - loss: 0.6693 - acc: 0.7684 - val_loss: 0.6558 - val_acc: 0.7764\n",
      "Epoch 50/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.6945 - acc: 0.7544Epoch 00049: val_loss did not improve\n",
      "3904/3868 [==============================] - 166s - loss: 0.6922 - acc: 0.7549 - val_loss: 0.6525 - val_acc: 0.7871\n",
      "Epoch 51/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.7055 - acc: 0.7521Epoch 00050: val_loss did not improve\n",
      "3924/3868 [==============================] - 165s - loss: 0.7062 - acc: 0.7510 - val_loss: 0.6085 - val_acc: 0.7891\n",
      "Epoch 52/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.6800 - acc: 0.7617Epoch 00051: val_loss did not improve\n",
      "\n",
      "Epoch 00051: reducing learning rate to 9.999999747378752e-07.\n",
      "3904/3868 [==============================] - 165s - loss: 0.6802 - acc: 0.7608 - val_loss: 0.6289 - val_acc: 0.7764\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f010ed69048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resnet50\n",
    "#stg1 training\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "\n",
    "#x = MaxPooling2D()(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(512, activation='relu', init='glorot_normal')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = LeakyReLU(alpha=0.33)(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "#x = Dense(256, init='glorot_normal', activation='relu')(x)\n",
    "#x = LeakyReLU(alpha=0.33)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional VGG16 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoints file ./checkpoints/weights.040-0.6054.hdf5\n",
      "0 input_2\n",
      "1 zeropadding2d_2\n",
      "2 conv1\n",
      "3 bn_conv1\n",
      "4 activation_50\n",
      "5 maxpooling2d_2\n",
      "6 res2a_branch2a\n",
      "7 bn2a_branch2a\n",
      "8 activation_51\n",
      "9 res2a_branch2b\n",
      "10 bn2a_branch2b\n",
      "11 activation_52\n",
      "12 res2a_branch2c\n",
      "13 res2a_branch1\n",
      "14 bn2a_branch2c\n",
      "15 bn2a_branch1\n",
      "16 merge_17\n",
      "17 activation_53\n",
      "18 res2b_branch2a\n",
      "19 bn2b_branch2a\n",
      "20 activation_54\n",
      "21 res2b_branch2b\n",
      "22 bn2b_branch2b\n",
      "23 activation_55\n",
      "24 res2b_branch2c\n",
      "25 bn2b_branch2c\n",
      "26 merge_18\n",
      "27 activation_56\n",
      "28 res2c_branch2a\n",
      "29 bn2c_branch2a\n",
      "30 activation_57\n",
      "31 res2c_branch2b\n",
      "32 bn2c_branch2b\n",
      "33 activation_58\n",
      "34 res2c_branch2c\n",
      "35 bn2c_branch2c\n",
      "36 merge_19\n",
      "37 activation_59\n",
      "38 res3a_branch2a\n",
      "39 bn3a_branch2a\n",
      "40 activation_60\n",
      "41 res3a_branch2b\n",
      "42 bn3a_branch2b\n",
      "43 activation_61\n",
      "44 res3a_branch2c\n",
      "45 res3a_branch1\n",
      "46 bn3a_branch2c\n",
      "47 bn3a_branch1\n",
      "48 merge_20\n",
      "49 activation_62\n",
      "50 res3b_branch2a\n",
      "51 bn3b_branch2a\n",
      "52 activation_63\n",
      "53 res3b_branch2b\n",
      "54 bn3b_branch2b\n",
      "55 activation_64\n",
      "56 res3b_branch2c\n",
      "57 bn3b_branch2c\n",
      "58 merge_21\n",
      "59 activation_65\n",
      "60 res3c_branch2a\n",
      "61 bn3c_branch2a\n",
      "62 activation_66\n",
      "63 res3c_branch2b\n",
      "64 bn3c_branch2b\n",
      "65 activation_67\n",
      "66 res3c_branch2c\n",
      "67 bn3c_branch2c\n",
      "68 merge_22\n",
      "69 activation_68\n",
      "70 res3d_branch2a\n",
      "71 bn3d_branch2a\n",
      "72 activation_69\n",
      "73 res3d_branch2b\n",
      "74 bn3d_branch2b\n",
      "75 activation_70\n",
      "76 res3d_branch2c\n",
      "77 bn3d_branch2c\n",
      "78 merge_23\n",
      "79 activation_71\n",
      "80 res4a_branch2a\n",
      "81 bn4a_branch2a\n",
      "82 activation_72\n",
      "83 res4a_branch2b\n",
      "84 bn4a_branch2b\n",
      "85 activation_73\n",
      "86 res4a_branch2c\n",
      "87 res4a_branch1\n",
      "88 bn4a_branch2c\n",
      "89 bn4a_branch1\n",
      "90 merge_24\n",
      "91 activation_74\n",
      "92 res4b_branch2a\n",
      "93 bn4b_branch2a\n",
      "94 activation_75\n",
      "95 res4b_branch2b\n",
      "96 bn4b_branch2b\n",
      "97 activation_76\n",
      "98 res4b_branch2c\n",
      "99 bn4b_branch2c\n",
      "100 merge_25\n",
      "101 activation_77\n",
      "102 res4c_branch2a\n",
      "103 bn4c_branch2a\n",
      "104 activation_78\n",
      "105 res4c_branch2b\n",
      "106 bn4c_branch2b\n",
      "107 activation_79\n",
      "108 res4c_branch2c\n",
      "109 bn4c_branch2c\n",
      "110 merge_26\n",
      "111 activation_80\n",
      "112 res4d_branch2a\n",
      "113 bn4d_branch2a\n",
      "114 activation_81\n",
      "115 res4d_branch2b\n",
      "116 bn4d_branch2b\n",
      "117 activation_82\n",
      "118 res4d_branch2c\n",
      "119 bn4d_branch2c\n",
      "120 merge_27\n",
      "121 activation_83\n",
      "122 res4e_branch2a\n",
      "123 bn4e_branch2a\n",
      "124 activation_84\n",
      "125 res4e_branch2b\n",
      "126 bn4e_branch2b\n",
      "127 activation_85\n",
      "128 res4e_branch2c\n",
      "129 bn4e_branch2c\n",
      "130 merge_28\n",
      "131 activation_86\n",
      "132 res4f_branch2a\n",
      "133 bn4f_branch2a\n",
      "134 activation_87\n",
      "135 res4f_branch2b\n",
      "136 bn4f_branch2b\n",
      "137 activation_88\n",
      "138 res4f_branch2c\n",
      "139 bn4f_branch2c\n",
      "140 merge_29\n",
      "141 activation_89\n",
      "142 res5a_branch2a\n",
      "143 bn5a_branch2a\n",
      "144 activation_90\n",
      "145 res5a_branch2b\n",
      "146 bn5a_branch2b\n",
      "147 activation_91\n",
      "148 res5a_branch2c\n",
      "149 res5a_branch1\n",
      "150 bn5a_branch2c\n",
      "151 bn5a_branch1\n",
      "152 merge_30\n",
      "153 activation_92\n",
      "154 res5b_branch2a\n",
      "155 bn5b_branch2a\n",
      "156 activation_93\n",
      "157 res5b_branch2b\n",
      "158 bn5b_branch2b\n",
      "159 activation_94\n",
      "160 res5b_branch2c\n",
      "161 bn5b_branch2c\n",
      "162 merge_31\n",
      "163 activation_95\n",
      "164 res5c_branch2a\n",
      "165 bn5c_branch2a\n",
      "166 activation_96\n",
      "167 res5c_branch2b\n",
      "168 bn5c_branch2b\n",
      "169 activation_97\n",
      "170 res5c_branch2c\n",
      "171 bn5c_branch2c\n",
      "172 merge_32\n",
      "173 activation_98\n",
      "174 avg_pool\n",
      "Epoch 1/300\n",
      "3840/3868 [============================>.] - ETA: 1s - loss: 0.6381 - acc: 0.7789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from 0.60545 to 0.56201, saving model to ./checkpoints/weights.000-0.5620.hdf5\n",
      "3904/3868 [==============================] - 188s - loss: 0.6358 - acc: 0.7800 - val_loss: 0.5620 - val_acc: 0.8115\n",
      "Epoch 2/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.5396 - acc: 0.8190Epoch 00001: val_loss improved from 0.56201 to 0.55127, saving model to ./checkpoints/weights.001-0.5513.hdf5\n",
      "3904/3868 [==============================] - 172s - loss: 0.5393 - acc: 0.8179 - val_loss: 0.5513 - val_acc: 0.8154\n",
      "Epoch 3/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.4609 - acc: 0.8402Epoch 00002: val_loss improved from 0.55127 to 0.43090, saving model to ./checkpoints/weights.002-0.4309.hdf5\n",
      "3924/3868 [==============================] - 174s - loss: 0.4591 - acc: 0.8415 - val_loss: 0.4309 - val_acc: 0.8535\n",
      "Epoch 4/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8789Epoch 00003: val_loss did not improve\n",
      "3904/3868 [==============================] - 172s - loss: 0.3630 - acc: 0.8788 - val_loss: 0.4506 - val_acc: 0.8496\n",
      "Epoch 5/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.3026 - acc: 0.8956Epoch 00004: val_loss improved from 0.43090 to 0.39957, saving model to ./checkpoints/weights.004-0.3996.hdf5\n",
      "3904/3868 [==============================] - 172s - loss: 0.3015 - acc: 0.8960 - val_loss: 0.3996 - val_acc: 0.8691\n",
      "Epoch 6/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.2612 - acc: 0.9122Epoch 00005: val_loss improved from 0.39957 to 0.38710, saving model to ./checkpoints/weights.005-0.3871.hdf5\n",
      "3924/3868 [==============================] - 173s - loss: 0.2616 - acc: 0.9118 - val_loss: 0.3871 - val_acc: 0.8779\n",
      "Epoch 7/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.2176 - acc: 0.9284Epoch 00006: val_loss improved from 0.38710 to 0.36136, saving model to ./checkpoints/weights.006-0.3614.hdf5\n",
      "3904/3868 [==============================] - 173s - loss: 0.2157 - acc: 0.9293 - val_loss: 0.3614 - val_acc: 0.8936\n",
      "Epoch 8/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9380Epoch 00007: val_loss improved from 0.36136 to 0.32231, saving model to ./checkpoints/weights.007-0.3223.hdf5\n",
      "3904/3868 [==============================] - 172s - loss: 0.1819 - acc: 0.9380 - val_loss: 0.3223 - val_acc: 0.9102\n",
      "Epoch 9/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.1744 - acc: 0.9484Epoch 00008: val_loss improved from 0.32231 to 0.28482, saving model to ./checkpoints/weights.008-0.2848.hdf5\n",
      "3924/3868 [==============================] - 172s - loss: 0.1732 - acc: 0.9488 - val_loss: 0.2848 - val_acc: 0.9160\n",
      "Epoch 10/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9539Epoch 00009: val_loss improved from 0.28482 to 0.25652, saving model to ./checkpoints/weights.009-0.2565.hdf5\n",
      "3904/3868 [==============================] - 172s - loss: 0.1366 - acc: 0.9539 - val_loss: 0.2565 - val_acc: 0.9297\n",
      "Epoch 11/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.1272 - acc: 0.9581Epoch 00010: val_loss did not improve\n",
      "3904/3868 [==============================] - 172s - loss: 0.1277 - acc: 0.9580 - val_loss: 0.2724 - val_acc: 0.9277\n",
      "Epoch 12/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.1288 - acc: 0.9570Epoch 00011: val_loss did not improve\n",
      "3924/3868 [==============================] - 172s - loss: 0.1286 - acc: 0.9569 - val_loss: 0.3084 - val_acc: 0.9111\n",
      "Epoch 13/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.1028 - acc: 0.9648Epoch 00012: val_loss improved from 0.25652 to 0.24134, saving model to ./checkpoints/weights.012-0.2413.hdf5\n",
      "3904/3868 [==============================] - 172s - loss: 0.1032 - acc: 0.9644 - val_loss: 0.2413 - val_acc: 0.9414\n",
      "Epoch 14/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9661Epoch 00013: val_loss did not improve\n",
      "3904/3868 [==============================] - 173s - loss: 0.1024 - acc: 0.9659 - val_loss: 0.2967 - val_acc: 0.9229\n",
      "Epoch 15/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.0892 - acc: 0.9705Epoch 00014: val_loss did not improve\n",
      "3924/3868 [==============================] - 173s - loss: 0.0903 - acc: 0.9704 - val_loss: 0.3723 - val_acc: 0.9092\n",
      "Epoch 16/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0811 - acc: 0.9745Epoch 00015: val_loss improved from 0.24134 to 0.22489, saving model to ./checkpoints/weights.015-0.2249.hdf5\n",
      "3904/3868 [==============================] - 172s - loss: 0.0821 - acc: 0.9746 - val_loss: 0.2249 - val_acc: 0.9424\n",
      "Epoch 17/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9677Epoch 00016: val_loss did not improve\n",
      "3904/3868 [==============================] - 172s - loss: 0.0805 - acc: 0.9682 - val_loss: 0.3126 - val_acc: 0.9229\n",
      "Epoch 18/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9790Epoch 00017: val_loss improved from 0.22489 to 0.19330, saving model to ./checkpoints/weights.017-0.1933.hdf5\n",
      "3924/3868 [==============================] - 174s - loss: 0.0671 - acc: 0.9791 - val_loss: 0.1933 - val_acc: 0.9434\n",
      "Epoch 19/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9810Epoch 00018: val_loss did not improve\n",
      "3904/3868 [==============================] - 171s - loss: 0.0583 - acc: 0.9805 - val_loss: 0.2391 - val_acc: 0.9443\n",
      "Epoch 20/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9760Epoch 00019: val_loss did not improve\n",
      "3904/3868 [==============================] - 172s - loss: 0.0682 - acc: 0.9764 - val_loss: 0.2257 - val_acc: 0.9492\n",
      "Epoch 21/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9767Epoch 00020: val_loss did not improve\n",
      "3924/3868 [==============================] - 173s - loss: 0.0706 - acc: 0.9771 - val_loss: 0.2111 - val_acc: 0.9502\n",
      "Epoch 22/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9828Epoch 00021: val_loss did not improve\n",
      "3904/3868 [==============================] - 171s - loss: 0.0512 - acc: 0.9828 - val_loss: 0.2299 - val_acc: 0.9395\n",
      "Epoch 23/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9836Epoch 00022: val_loss did not improve\n",
      "3904/3868 [==============================] - 171s - loss: 0.0495 - acc: 0.9836 - val_loss: 0.3130 - val_acc: 0.9365\n",
      "Epoch 24/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9852Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 9.999999747378752e-06.\n",
      "3924/3868 [==============================] - 173s - loss: 0.0503 - acc: 0.9852 - val_loss: 0.2885 - val_acc: 0.9404\n",
      "Epoch 25/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9826Epoch 00024: val_loss did not improve\n",
      "3904/3868 [==============================] - 173s - loss: 0.0519 - acc: 0.9828 - val_loss: 0.2361 - val_acc: 0.9453\n",
      "Epoch 26/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9872Epoch 00025: val_loss did not improve\n",
      "3904/3868 [==============================] - 173s - loss: 0.0415 - acc: 0.9874 - val_loss: 0.2471 - val_acc: 0.9531\n",
      "Epoch 27/300\n",
      "3860/3868 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9868Epoch 00026: val_loss did not improve\n",
      "3924/3868 [==============================] - 173s - loss: 0.0404 - acc: 0.9870 - val_loss: 0.2242 - val_acc: 0.9531\n",
      "Epoch 28/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9914Epoch 00027: val_loss did not improve\n",
      "3904/3868 [==============================] - 171s - loss: 0.0305 - acc: 0.9915 - val_loss: 0.2076 - val_acc: 0.9541\n",
      "Epoch 29/300\n",
      "3840/3868 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9919Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 9.999999747378752e-07.\n",
      "3904/3868 [==============================] - 171s - loss: 0.0315 - acc: 0.9918 - val_loss: 0.2541 - val_acc: 0.9521\n",
      "Epoch 00028: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00fa0b8f28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resnet50\n",
    "#stg2 training\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 172 layers and unfreeze the rest:\n",
    "for layer in model.layers[:164]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[164:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#resume training\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from checkpoints file ' + files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5940\n"
     ]
    }
   ],
   "source": [
    "#get bbox from detections_full.pkl and RFCN/ImageSets/Main/test.txt\n",
    "FISH_CLASSES = ['NoF', 'ALB', 'BET', 'DOL', 'LAG', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "import pickle \n",
    "with open('../data/RFNC_detections/detections_full.pkl','rb') as f:\n",
    "    detections_full = pickle.load(f, encoding='latin1') \n",
    "    \n",
    "CONF_THRESH = 0.8\n",
    "outputs = []\n",
    "\n",
    "for im in range(len(detections_full[0])):\n",
    "#for im in range(1):\n",
    "    outputs_im = []\n",
    "    detects_im = []\n",
    "    for cls in range(1,len(FISH_CLASSES)):\n",
    "        detects_im_cls = detections_full[cls][im]\n",
    "        for i in range(len(detects_im_cls)):\n",
    "            if np.max(detects_im_cls[i,4+cls]) >= CONF_THRESH:\n",
    "                outputs_im.append(detects_im_cls[i,:])   \n",
    "    for cls in range(1,len(FISH_CLASSES)):  \n",
    "        detects_im.append(detections_full[cls][im])\n",
    "    detects_im = np.vstack(detects_im)\n",
    "    if len(outputs_im) == 0:\n",
    "        ind = np.argmax(np.max(detects_im[:,5:], axis=1))\n",
    "        l = [0,0,0,0]\n",
    "        l.extend(np.ndarray.tolist(detects_im[ind,4:]))\n",
    "        outputs_im.append(l)\n",
    "    outputs_im = np.asarray(outputs_im)\n",
    "    outputs.append(outputs_im)\n",
    "\n",
    "print(sum([outputs[i].shape[0] for i in range(len(outputs))]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#crop test images and cache to TEST_CROP_DIR\n",
    "if not os.path.exists(TEST_CROP_DIR):\n",
    "    os.mkdir(TEST_CROP_DIR)\n",
    "files = glob.glob(TEST_CROP_DIR+'*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "with open(\"../RFCN/ImageSets/Main/test.txt\",\"r\") as f:\n",
    "    ims = f.readlines()\n",
    "test_files = [im[:-1]+'.jpg' for im in ims]\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    filename = test_files[i]\n",
    "    print(i,)\n",
    "    bboxes = outputs[i]\n",
    "    basename, file_extension = os.path.splitext(filename) \n",
    "    image = Image.open(TEST_DIR+filename)\n",
    "    for j in range(len(bboxes)):\n",
    "        bbox = bboxes[j]\n",
    "        xmin = bbox[0]\n",
    "        ymin = bbox[1]\n",
    "        xmax = bbox[2]\n",
    "        ymax = bbox[3]\n",
    "        file_crop = TEST_CROP_DIR+basename+'_{}_'.format(j)+'.jpg'\n",
    "        if xmin==0 and ymin==0 and xmax==0 and ymax==0:\n",
    "            cropped = image\n",
    "        else:\n",
    "            #save cropped img\n",
    "            cropped = image.crop((xmin, ymin, xmax, ymax))\n",
    "            #cropped = image[max(ymin,0):ymax, max(xmin,0):xmax]\n",
    "        width_cropped, height_cropped = cropped.size\n",
    "        if height_cropped > width_cropped: cropped = cropped.transpose(method=2)\n",
    "        cropped.save(file_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from original images. Generating data_test_BBCrop_270_480.pickle.\n",
      "Processed 0 of 5940\n",
      "Processed 1000 of 5940\n",
      "Processed 2000 of 5940\n",
      "Processed 3000 of 5940\n",
      "Processed 4000 of 5940\n",
      "Processed 5000 of 5940\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fd397e2014a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mX_test_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_crop\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Load test data\n",
    "\n",
    "import datetime\n",
    "\n",
    "def read_image(src):\n",
    "    \"\"\"Read and resize individual images\"\"\"\n",
    "    im = Image.open(src)\n",
    "    im = im.resize((COLS, ROWS), Image.BILINEAR)\n",
    "    im = np.asarray(im)\n",
    "    return im\n",
    "\n",
    "if os.path.exists('../data/data_test_BBCrop_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_test_BBCrop_{}_{}.pickle. Loading test data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_test_BBCrop_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_test = pickle.load(f)\n",
    "    X_test_crop = data_test['X_test_crop']\n",
    "    test_crop_files = data_test['test_crop_files']\n",
    "else:\n",
    "    print ('Loading test data from original images. Generating data_test_BBCrop_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    test_crop_files = [im for im in os.listdir(TEST_CROP_DIR)]\n",
    "    X_test_crop = np.ndarray((len(test_crop_files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(test_crop_files): \n",
    "        X_test_crop[i] = read_image(TEST_CROP_DIR+im)\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, len(test_crop_files)))\n",
    "            \n",
    "    data_test = {'X_test_crop': X_test_crop,'test_crop_files': test_crop_files }\n",
    "    \n",
    "    with open('../data/data_test_BBCrop_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_test, f, protocol=4)\n",
    "        \n",
    "X_test_crop = X_test_crop / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./checkpoints/weights.017-0.1933.hdf5\n",
      "8216/8216 [==============================] - 231s   \n"
     ]
    }
   ],
   "source": [
    "#test preds\n",
    "\n",
    "files = glob.glob('./checkpoints/*')\n",
    "val_losses = [float(f.split('-')[-1][:-5]) for f in files]\n",
    "index = val_losses.index(min(val_losses))\n",
    "print('Loading model from', files[index])\n",
    "model = load_model(files[index])\n",
    "\n",
    "test_crop_preds = model.predict(X_test_crop, batch_size=BatchSize, verbose=1)\n",
    "\n",
    "with open(\"../RFCN/ImageSets/Main/test.txt\",\"r\") as f:\n",
    "    ims = f.readlines()\n",
    "test_files = [im[:-1]+'.jpg' for im in ims]\n",
    "\n",
    "test_preds = np.ndarray((len(test_files), test_crop_preds.shape[1]), dtype=np.float32)\n",
    "for j in range(len(test_files)):\n",
    "    file = test_files[j]\n",
    "    test_preds_im = []\n",
    "    for i in range(len(test_crop_files)):\n",
    "        if test_crop_files[i][:9] == file[:9]:\n",
    "            test_preds_im.append(test_crop_preds[i])\n",
    "    test_preds_im = np.asarray(test_preds_im)\n",
    "    test_preds_im = np.mean(test_preds_im, axis=0)\n",
    "    test_preds[j] = test_preds_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3777\n",
      "logloss of train is 0.692274711996035\n"
     ]
    }
   ],
   "source": [
    "#calculate train logloss\n",
    "train_files = test_files[1000:]\n",
    "train_preds = test_preds[1000:,:]\n",
    "with open(\"../RFCN/ImageSets/Main/train_test.txt\",\"r\") as f:\n",
    "    train_file_labels = f.readlines()\n",
    "\n",
    "log_losses = []\n",
    "for i in range(len(train_preds)):\n",
    "    im = train_files[i][:-4]\n",
    "    for im_label in train_file_labels:\n",
    "        if im_label[:9] == im:\n",
    "            label = im_label[10:-1]\n",
    "            index = FISH_CLASSES.index(label)\n",
    "            log_losses.append(-math.log(train_preds[i,index]))\n",
    "log_loss = sum(log_losses) / float(len(log_losses))\n",
    "print('logloss of train is', log_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test submission\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "submission = pd.DataFrame(test_preds[:1000,:], columns=FISH_CLASSES)\n",
    "submission.insert(0, 'image', test_files[:1000])\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "info = 'BBCROP_resnet50_' + '{:.4f}'.format(log_loss)\n",
    "sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "submission.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###clear checkpoints folder\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "files = glob.glob('./checkpoints/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###clear logs folder\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')\n",
    "files = glob.glob('./logs/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
