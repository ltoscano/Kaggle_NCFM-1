{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, BatchNormalization, LeakyReLU, AveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "TRAIN_DIR = '../data/train/'\n",
    "TEST_DIR = '../data/test_stg1/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "ROWS = 256\n",
    "COLS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data_train_256_256.pickle.\n",
      "Processed 0 of 3777\n",
      "Processed 1000 of 3777\n",
      "Processed 2000 of 3777\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data_train_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_train_{}_{}.pickle. Loading from data file.'.format(ROWS, COLS))\n",
    "    with open('data_train_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    x_train = data_train['x_train']\n",
    "    y_train = data_train['y_train']\n",
    "else:\n",
    "    print ('Generating data_train_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    def get_images(fish):\n",
    "        \"\"\"Load files from train folder\"\"\"\n",
    "        fish_dir = TRAIN_DIR+'{}'.format(fish)\n",
    "        images = [fish+'/'+im for im in os.listdir(fish_dir)]\n",
    "        return images\n",
    "\n",
    "    def read_image(src):\n",
    "        \"\"\"Read and resize individual images\"\"\"\n",
    "        im = Image.open(src)\n",
    "        im = im.resize((COLS, ROWS), Image.BILINEAR)\n",
    "        im = np.asarray(im)\n",
    "        return im\n",
    "\n",
    "    files = []\n",
    "    y_train = []\n",
    "\n",
    "    for fish in FISH_CLASSES:\n",
    "        fish_files = get_images(fish)\n",
    "        files.extend(fish_files)\n",
    "\n",
    "        y_fish = np.tile(fish, len(fish_files))\n",
    "        y_train.extend(y_fish)\n",
    "        #print(\"{0} photos of {1}\".format(len(fish_files), fish))\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    x_train = np.ndarray((len(files), ROWS, COLS, 3), dtype=np.float32)\n",
    "\n",
    "    for i, im in enumerate(files): \n",
    "        x_train[i] = read_image(TRAIN_DIR+im)\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "    x_train = x_train / 255\n",
    "    #print(x_train.shape)\n",
    "\n",
    "    # One Hot Encoding Labels\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "    #save data to file\n",
    "    import pickle\n",
    "\n",
    "    data_train = {'x_train': x_train,'y_train': y_train }\n",
    "\n",
    "    with open('data_train_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_train, f)\n",
    "\n",
    "#x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=22, stratify=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[253 254 253]\n",
      "   [253 254 252]\n",
      "   [252 254 252]\n",
      "   ..., \n",
      "   [ 98  93  90]\n",
      "   [ 98  93  89]\n",
      "   [ 99  94  90]]\n",
      "\n",
      "  [[228 228 224]\n",
      "   [187 187 189]\n",
      "   [197 195 194]\n",
      "   ..., \n",
      "   [100  95  91]\n",
      "   [101  96  92]\n",
      "   [102  97  93]]\n",
      "\n",
      "  [[217 215 210]\n",
      "   [164 164 165]\n",
      "   [184 182 184]\n",
      "   ..., \n",
      "   [101  96  92]\n",
      "   [102  97  93]\n",
      "   [103  98  94]]\n",
      "\n",
      "  ..., \n",
      "  [[ 46  55  70]\n",
      "   [ 51  60  75]\n",
      "   [ 67  76  91]\n",
      "   ..., \n",
      "   [ 57  66  85]\n",
      "   [ 50  60  79]\n",
      "   [ 49  59  81]]\n",
      "\n",
      "  [[ 46  55  69]\n",
      "   [ 49  59  72]\n",
      "   [ 63  72  86]\n",
      "   ..., \n",
      "   [ 63  72  81]\n",
      "   [ 57  66  79]\n",
      "   [ 53  61  80]]\n",
      "\n",
      "  [[ 46  56  68]\n",
      "   [ 48  58  70]\n",
      "   [ 56  66  78]\n",
      "   ..., \n",
      "   [ 57  70  77]\n",
      "   [ 61  71  84]\n",
      "   [ 61  68  89]]]\n",
      "\n",
      "\n",
      " [[[253 254 253]\n",
      "   [253 254 252]\n",
      "   [252 254 252]\n",
      "   ..., \n",
      "   [253 252 251]\n",
      "   [252 252 252]\n",
      "   [252 252 252]]\n",
      "\n",
      "  [[228 228 224]\n",
      "   [187 187 189]\n",
      "   [197 195 194]\n",
      "   ..., \n",
      "   [237 241 222]\n",
      "   [248 251 237]\n",
      "   [250 253 241]]\n",
      "\n",
      "  [[217 215 210]\n",
      "   [164 164 165]\n",
      "   [184 182 184]\n",
      "   ..., \n",
      "   [105 115 103]\n",
      "   [137 146 134]\n",
      "   [177 186 172]]\n",
      "\n",
      "  ..., \n",
      "  [[ 74  75  69]\n",
      "   [ 79  76  72]\n",
      "   [ 85  80  78]\n",
      "   ..., \n",
      "   [ 68  69  61]\n",
      "   [ 71  72  64]\n",
      "   [ 76  77  69]]\n",
      "\n",
      "  [[ 72  73  68]\n",
      "   [ 77  74  70]\n",
      "   [ 83  78  76]\n",
      "   ..., \n",
      "   [ 68  69  61]\n",
      "   [ 71  72  64]\n",
      "   [ 74  75  67]]\n",
      "\n",
      "  [[ 73  73  68]\n",
      "   [ 77  74  70]\n",
      "   [ 83  77  76]\n",
      "   ..., \n",
      "   [ 68  69  61]\n",
      "   [ 70  71  63]\n",
      "   [ 71  72  64]]]\n",
      "\n",
      "\n",
      " [[[209 219 205]\n",
      "   [222 231 217]\n",
      "   [222 230 217]\n",
      "   ..., \n",
      "   [194 211 217]\n",
      "   [146 163 170]\n",
      "   [ 89 106 113]]\n",
      "\n",
      "  [[226 233 218]\n",
      "   [224 233 218]\n",
      "   [227 237 221]\n",
      "   ..., \n",
      "   [170 187 193]\n",
      "   [185 202 208]\n",
      "   [106 123 130]]\n",
      "\n",
      "  [[227 236 219]\n",
      "   [228 238 220]\n",
      "   [235 244 227]\n",
      "   ..., \n",
      "   [ 67  84  91]\n",
      "   [171 188 195]\n",
      "   [151 168 175]]\n",
      "\n",
      "  ..., \n",
      "  [[254 251 247]\n",
      "   [242 240 238]\n",
      "   [249 249 248]\n",
      "   ..., \n",
      "   [ 92 114 120]\n",
      "   [ 98 121 127]\n",
      "   [ 66  89  95]]\n",
      "\n",
      "  [[248 242 234]\n",
      "   [221 216 212]\n",
      "   [241 238 239]\n",
      "   ..., \n",
      "   [ 99 121 127]\n",
      "   [ 99 122 128]\n",
      "   [ 71  94 100]]\n",
      "\n",
      "  [[241 233 222]\n",
      "   [216 210 204]\n",
      "   [243 239 240]\n",
      "   ..., \n",
      "   [ 98 121 126]\n",
      "   [ 81 104 110]\n",
      "   [ 63  86  92]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[172 177 173]\n",
      "   [167 172 168]\n",
      "   [165 170 166]\n",
      "   ..., \n",
      "   [217 214 199]\n",
      "   [219 216 201]\n",
      "   [219 216 201]]\n",
      "\n",
      "  [[169 174 170]\n",
      "   [166 171 167]\n",
      "   [164 169 165]\n",
      "   ..., \n",
      "   [217 214 199]\n",
      "   [218 215 200]\n",
      "   [219 216 201]]\n",
      "\n",
      "  [[167 172 168]\n",
      "   [165 170 166]\n",
      "   [163 168 164]\n",
      "   ..., \n",
      "   [216 213 198]\n",
      "   [214 211 196]\n",
      "   [220 217 202]]\n",
      "\n",
      "  ..., \n",
      "  [[ 70  81  83]\n",
      "   [ 69  80  85]\n",
      "   [ 69  79  88]\n",
      "   ..., \n",
      "   [119  94  64]\n",
      "   [120  95  65]\n",
      "   [121  96  66]]\n",
      "\n",
      "  [[ 67  78  80]\n",
      "   [ 67  78  83]\n",
      "   [ 68  78  86]\n",
      "   ..., \n",
      "   [118  93  63]\n",
      "   [120  95  65]\n",
      "   [121  96  66]]\n",
      "\n",
      "  [[ 62  73  75]\n",
      "   [ 64  75  79]\n",
      "   [ 67  77  85]\n",
      "   ..., \n",
      "   [118  93  63]\n",
      "   [120  95  65]\n",
      "   [121  96  66]]]\n",
      "\n",
      "\n",
      " [[[ 49  53  52]\n",
      "   [ 51  55  54]\n",
      "   [ 53  57  56]\n",
      "   ..., \n",
      "   [113 131 139]\n",
      "   [115 132 147]\n",
      "   [105 122 143]]\n",
      "\n",
      "  [[ 49  53  52]\n",
      "   [ 51  55  54]\n",
      "   [ 53  57  56]\n",
      "   ..., \n",
      "   [111 129 139]\n",
      "   [110 128 142]\n",
      "   [110 127 146]]\n",
      "\n",
      "  [[ 49  53  52]\n",
      "   [ 51  55  54]\n",
      "   [ 52  56  55]\n",
      "   ..., \n",
      "   [122 140 151]\n",
      "   [101 119 133]\n",
      "   [117 134 151]]\n",
      "\n",
      "  ..., \n",
      "  [[ 27  31  34]\n",
      "   [ 24  28  31]\n",
      "   [ 25  29  32]\n",
      "   ..., \n",
      "   [  7  36  62]\n",
      "   [  5  31  58]\n",
      "   [  2  27  55]]\n",
      "\n",
      "  [[ 54  58  61]\n",
      "   [ 32  36  39]\n",
      "   [ 26  30  33]\n",
      "   ..., \n",
      "   [  9  38  64]\n",
      "   [  5  31  58]\n",
      "   [  3  27  55]]\n",
      "\n",
      "  [[ 48  51  54]\n",
      "   [ 22  26  29]\n",
      "   [ 20  24  27]\n",
      "   ..., \n",
      "   [  9  38  64]\n",
      "   [  5  31  58]\n",
      "   [  3  27  55]]]\n",
      "\n",
      "\n",
      " [[[253 254 253]\n",
      "   [253 254 252]\n",
      "   [252 254 252]\n",
      "   ..., \n",
      "   [101 106 110]\n",
      "   [102 107 111]\n",
      "   [108 113 117]]\n",
      "\n",
      "  [[228 228 224]\n",
      "   [187 187 189]\n",
      "   [197 195 194]\n",
      "   ..., \n",
      "   [103 106 111]\n",
      "   [103 107 112]\n",
      "   [110 114 118]]\n",
      "\n",
      "  [[217 215 210]\n",
      "   [164 164 165]\n",
      "   [184 182 184]\n",
      "   ..., \n",
      "   [104 107 112]\n",
      "   [105 108 113]\n",
      "   [112 115 120]]\n",
      "\n",
      "  ..., \n",
      "  [[ 86 104 114]\n",
      "   [ 87 105 115]\n",
      "   [ 89 107 117]\n",
      "   ..., \n",
      "   [ 46  59  94]\n",
      "   [ 36  57  88]\n",
      "   [ 35  56  93]]\n",
      "\n",
      "  [[ 88 107 116]\n",
      "   [ 89 107 116]\n",
      "   [ 90 108 117]\n",
      "   ..., \n",
      "   [ 57  62  83]\n",
      "   [ 46  58  85]\n",
      "   [ 42  58  89]]\n",
      "\n",
      "  [[ 92 111 118]\n",
      "   [ 92 111 118]\n",
      "   [ 92 111 118]\n",
      "   ..., \n",
      "   [ 60  62  84]\n",
      "   [ 51  61  87]\n",
      "   [ 45  62  91]]]]\n"
     ]
    }
   ],
   "source": [
    "print X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=1e-4, decay=0.0)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same', dim_ordering='tf'), input_shape=(ROWS, COLS, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same', dim_ordering='tf'), subsample=(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering='tf'), subsample=(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(128, 3, 3, border_mode='same', dim_ordering='tf'), subsample=(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', dim_ordering='tf'), subsample=(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, border_mode='same', dim_ordering='tf'), subsample=(2, 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=(7, 7), dim_ordering='tf'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(len(FISH_CLASSES), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=categorical_accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
