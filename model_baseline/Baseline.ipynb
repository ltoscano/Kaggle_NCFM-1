{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, random, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, BatchNormalization, LeakyReLU, AveragePooling2D, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../data/train/'\n",
    "TEST_DIR = '../data/test_stg1/'\n",
    "FISH_CLASSES = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "ROWS = 256\n",
    "COLS = 256\n",
    "BatchSize = 64\n",
    "LearningRate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exist data_train_256_256.pickle. Loading data from file.\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "\n",
    "import pickle\n",
    "if os.path.exists('../data/data_train_{}_{}.pickle'.format(ROWS, COLS)):\n",
    "    print ('Exist data_train_{}_{}.pickle. Loading data from file.'.format(ROWS, COLS))\n",
    "    with open('../data/data_train_{}_{}.pickle'.format(ROWS, COLS), 'rb') as f:\n",
    "        data_train = pickle.load(f)\n",
    "    X_train = data_train['X_train']\n",
    "    y_train = data_train['y_train']\n",
    "else:\n",
    "    print ('Loading data from original images. Generating data_train_{}_{}.pickle.'.format(ROWS, COLS))\n",
    "\n",
    "    def get_images(fish):\n",
    "        \"\"\"Load files from train folder\"\"\"\n",
    "        fish_dir = TRAIN_DIR+'{}'.format(fish)\n",
    "        images = [fish+'/'+im for im in os.listdir(fish_dir)]\n",
    "        return images\n",
    "\n",
    "    def read_image(src):\n",
    "        \"\"\"Read and resize individual images\"\"\"\n",
    "        im = Image.open(src)\n",
    "        im = im.resize((COLS, ROWS), Image.BILINEAR)\n",
    "        im = np.asarray(im)\n",
    "        return im\n",
    "\n",
    "    files = []\n",
    "    y_train = []\n",
    "\n",
    "    for fish in FISH_CLASSES:\n",
    "        fish_files = get_images(fish)\n",
    "        files.extend(fish_files)\n",
    "\n",
    "        y_fish = np.tile(fish, len(fish_files))\n",
    "        y_train.extend(y_fish)\n",
    "        #print(\"{0} photos of {1}\".format(len(fish_files), fish))\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    X_train = np.ndarray((len(files), ROWS, COLS, 3), dtype=np.uint8)\n",
    "\n",
    "    for i, im in enumerate(files): \n",
    "        X_train[i] = read_image(TRAIN_DIR+im)\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "\n",
    "    #X_train = X_train / 255.\n",
    "    #print(X_train.shape)\n",
    "\n",
    "    # One Hot Encoding Labels\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "    #save data to file\n",
    "    data_train = {'X_train': X_train,'y_train': y_train }\n",
    "\n",
    "    with open('../data/data_train_{}_{}.pickle'.format(ROWS, COLS), 'wb') as f:\n",
    "        pickle.dump(data_train, f)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=None, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "\n",
    "optimizer = Adam(lr=LearningRate)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf', input_shape=(ROWS, COLS, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(32, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf', subsample=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(Convolution2D(64, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(64, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf', subsample=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "\n",
    "    model.add(Convolution2D(128, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(128, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(128, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf', subsample=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(Convolution2D(256, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf', subsample=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(Convolution2D(256, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    model.add(Convolution2D(256, 3, 3, init='he_normal', border_mode='same', dim_ordering='tf', subsample=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.33))\n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=(7, 7), dim_ordering='tf'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, init='glorot_normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, init='glorot_normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(FISH_CLASSES), init='glorot_normal', activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rescale=1./255,\n",
    "    rotation_range=180,\n",
    "    shear_range=np.pi/6.,\n",
    "    zoom_range=[1,1.1],\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "#train_datagen.fit(X_train)\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=BatchSize, shuffle=True, seed=None)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_generator = valid_datagen.flow(X_valid, y_valid, batch_size=BatchSize, shuffle=True, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')        \n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath='./checkpoints/weights.{epoch:03d}-{val_loss:.4f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "        \n",
    "learningrate_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=1, mode='auto', epsilon=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=10, write_graph=True, write_images=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training model\n",
    "\n",
    "model = create_model()\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: saving model to ./checkpoints/weights.000-1.3385.hdf5\n",
      "3072/3021 [==============================] - 128s - loss: 1.1814 - val_loss: 1.3385\n",
      "Epoch 2/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1508Epoch 00001: saving model to ./checkpoints/weights.001-1.2941.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1528 - val_loss: 1.2941\n",
      "Epoch 3/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.1576Epoch 00002: saving model to ./checkpoints/weights.002-1.4349.hdf5\n",
      "3047/3021 [==============================] - 116s - loss: 1.1576 - val_loss: 1.4349\n",
      "Epoch 4/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1537Epoch 00003: saving model to ./checkpoints/weights.003-1.2930.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1531 - val_loss: 1.2930\n",
      "Epoch 5/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1536Epoch 00004: saving model to ./checkpoints/weights.004-1.2238.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1523 - val_loss: 1.2238\n",
      "Epoch 6/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.1580Epoch 00005: saving model to ./checkpoints/weights.005-1.1594.hdf5\n",
      "3047/3021 [==============================] - 115s - loss: 1.1551 - val_loss: 1.1594\n",
      "Epoch 7/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1668Epoch 00006: saving model to ./checkpoints/weights.006-1.8434.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1684 - val_loss: 1.8434\n",
      "Epoch 8/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1183Epoch 00007: saving model to ./checkpoints/weights.007-1.1384.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1197 - val_loss: 1.1384\n",
      "Epoch 9/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0904Epoch 00008: saving model to ./checkpoints/weights.008-2.2835.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 1.0912 - val_loss: 2.2835\n",
      "Epoch 10/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1086Epoch 00009: saving model to ./checkpoints/weights.009-1.4164.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1069 - val_loss: 1.4164\n",
      "Epoch 11/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.1256Epoch 00010: saving model to ./checkpoints/weights.010-1.3308.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.1324 - val_loss: 1.3308\n",
      "Epoch 12/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0874Epoch 00011: saving model to ./checkpoints/weights.011-1.2192.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 1.0881 - val_loss: 1.2192\n",
      "Epoch 13/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0877Epoch 00012: saving model to ./checkpoints/weights.012-1.2953.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0892 - val_loss: 1.2953\n",
      "Epoch 14/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0805Epoch 00013: saving model to ./checkpoints/weights.013-1.3896.hdf5\n",
      "3072/3021 [==============================] - 116s - loss: 1.0847 - val_loss: 1.3896\n",
      "Epoch 15/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0456Epoch 00014: saving model to ./checkpoints/weights.014-1.3385.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 1.0455 - val_loss: 1.3385\n",
      "Epoch 16/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0780Epoch 00015: saving model to ./checkpoints/weights.015-1.4084.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0803 - val_loss: 1.4084\n",
      "Epoch 17/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0141Epoch 00016: saving model to ./checkpoints/weights.016-1.1181.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0152 - val_loss: 1.1181\n",
      "Epoch 18/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0893Epoch 00017: saving model to ./checkpoints/weights.017-1.3489.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 1.0949 - val_loss: 1.3489\n",
      "Epoch 19/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0815Epoch 00018: saving model to ./checkpoints/weights.018-1.2279.hdf5\n",
      "3072/3021 [==============================] - 116s - loss: 1.0791 - val_loss: 1.2279\n",
      "Epoch 20/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0281Epoch 00019: saving model to ./checkpoints/weights.019-1.2738.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0271 - val_loss: 1.2738\n",
      "Epoch 21/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0242Epoch 00020: saving model to ./checkpoints/weights.020-1.1242.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 1.0241 - val_loss: 1.1242\n",
      "Epoch 22/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0563Epoch 00021: saving model to ./checkpoints/weights.021-0.9563.hdf5\n",
      "3072/3021 [==============================] - 114s - loss: 1.0562 - val_loss: 0.9563\n",
      "Epoch 23/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0173Epoch 00022: saving model to ./checkpoints/weights.022-0.8937.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0145 - val_loss: 0.8937\n",
      "Epoch 24/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0429Epoch 00023: saving model to ./checkpoints/weights.023-1.0060.hdf5\n",
      "3047/3021 [==============================] - 115s - loss: 1.0399 - val_loss: 1.0060\n",
      "Epoch 25/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0476Epoch 00024: saving model to ./checkpoints/weights.024-1.1191.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0471 - val_loss: 1.1191\n",
      "Epoch 26/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0100Epoch 00025: saving model to ./checkpoints/weights.025-1.0770.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0125 - val_loss: 1.0770\n",
      "Epoch 27/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 1.0153Epoch 00026: saving model to ./checkpoints/weights.026-1.1044.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 1.0124 - val_loss: 1.1044\n",
      "Epoch 28/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 1.0246Epoch 00027: saving model to ./checkpoints/weights.027-1.0847.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 1.0230 - val_loss: 1.0847\n",
      "Epoch 29/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9440Epoch 00028: saving model to ./checkpoints/weights.028-1.1596.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 0.9486 - val_loss: 1.1596\n",
      "Epoch 30/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 0.9692Epoch 00029: saving model to ./checkpoints/weights.029-1.0633.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 0.9729 - val_loss: 1.0633\n",
      "Epoch 31/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9611Epoch 00030: saving model to ./checkpoints/weights.030-1.0917.hdf5\n",
      "3072/3021 [==============================] - 116s - loss: 0.9622 - val_loss: 1.0917\n",
      "Epoch 32/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9725Epoch 00031: saving model to ./checkpoints/weights.031-1.0636.hdf5\n",
      "3072/3021 [==============================] - 114s - loss: 0.9761 - val_loss: 1.0636\n",
      "Epoch 33/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 0.9997Epoch 00032: saving model to ./checkpoints/weights.032-1.1704.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 0.9998 - val_loss: 1.1704\n",
      "Epoch 34/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9619Epoch 00033: saving model to ./checkpoints/weights.033-0.8849.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 0.9592 - val_loss: 0.8849\n",
      "Epoch 35/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9455Epoch 00034: saving model to ./checkpoints/weights.034-0.9998.hdf5\n",
      "3072/3021 [==============================] - 116s - loss: 0.9464 - val_loss: 0.9998\n",
      "Epoch 36/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 0.9377Epoch 00035: saving model to ./checkpoints/weights.035-0.8383.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 0.9347 - val_loss: 0.8383\n",
      "Epoch 37/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9449Epoch 00036: saving model to ./checkpoints/weights.036-0.9094.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 0.9419 - val_loss: 0.9094\n",
      "Epoch 38/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9281Epoch 00037: saving model to ./checkpoints/weights.037-0.9129.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 0.9289 - val_loss: 0.9129\n",
      "Epoch 39/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 0.9560Epoch 00038: saving model to ./checkpoints/weights.038-1.0476.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 0.9540 - val_loss: 1.0476\n",
      "Epoch 40/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9227Epoch 00039: saving model to ./checkpoints/weights.039-0.8868.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 0.9177 - val_loss: 0.8868\n",
      "Epoch 41/300\n",
      "3008/3021 [============================>.] - ETA: 0s - loss: 0.9121Epoch 00040: saving model to ./checkpoints/weights.040-1.0474.hdf5\n",
      "3072/3021 [==============================] - 115s - loss: 0.9133 - val_loss: 1.0474\n",
      "Epoch 42/300\n",
      "2983/3021 [============================>.] - ETA: 1s - loss: 0.9496Epoch 00041: saving model to ./checkpoints/weights.041-1.1636.hdf5\n",
      "3047/3021 [==============================] - 114s - loss: 0.9513 - val_loss: 1.1636\n",
      "Epoch 43/300\n",
      "1088/3021 [=========>....................] - ETA: 65s - loss: 0.8769"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-69677d6617f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n\u001b[1;32m      8\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningrate_schedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1450\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1452\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#resume training\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./checkpoints/weights.022-1.0515.hdf5')\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(X_train), nb_epoch=300, verbose=1, \n",
    "                    callbacks=[early_stopping, model_checkpoint, learningrate_schedule, tensorboard], \n",
    "                    validation_data=valid_generator, nb_val_samples=len(X_valid), nb_worker=3, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test submission\n",
    "\n",
    "test_files = [im for im in os.listdir(TEST_DIR)]\n",
    "test = np.ndarray((len(test_files), ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "\n",
    "for i, im in enumerate(test_files): \n",
    "    test[i] = read_image(TEST_DIR+im) / 255.\n",
    "    \n",
    "test_preds = model.predict(x, batch_size=BatchSize, verbose=1)\n",
    "\n",
    "submission = pd.DataFrame(test_preds, columns=FISH_CLASSES)\n",
    "submission.insert(0, 'image', test_files)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###clear log an checkpoints folder\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')\n",
    "files = glob.glob('./checkpoints/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "#if not os.path.exists('./logs'):\n",
    "#    os.mkdir('./logs')\n",
    "files = glob.glob('./logs/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
